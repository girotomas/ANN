{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN22_03.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/girotomas/ANN/blob/master/ANN22_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "7OqekavpzCVe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "// TODO:\n",
        "    -ex2.6 !!!!!!!!! (put many more epochs)\n",
        "    - ex2.6 with fashion dataset !!!!!!!!! (put many more epochs)\n",
        "    - ex 3.2 change epoch to more than 100 and compute\n",
        "    - ex 3.2.5.2 answer question\n",
        "    - ex 4 add more epochs !\n",
        "    - ex 4.2 \n",
        "    - ex 4.2 with fashion\n",
        "    - ex 5.2\n",
        "    - ex 5.3\n",
        "    - ex 6.3 comparison of performance !\n",
        "    - ex 7.1 comment and compute\n",
        "    \n",
        "// QUESTIONS:\n",
        "    - does the softmax layer count as an output layer? How many layers has the model without hiddel layers\n",
        "    - where to put bias_regularizers??  !!\n",
        "    \n",
        "// OPTIONAL:\n",
        "    - ex 6.1 play with other models\n",
        "    "
      ]
    },
    {
      "metadata": {
        "id": "nXxhLiYltss5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Miniproject 1: Image Classification\n",
        "\n",
        "## Introduction\n",
        "\n",
        "### Important dates:\n",
        "\n",
        "- Project release: Friday, 15th March 2019\n",
        "- **Submission deadline**: Monday, 29th April 2019, 11:59 pm\n",
        "\n",
        "### Description\n",
        "\n",
        "One of the deepest traditions in learning about deep learning is to first [tackle the exciting problem of MNIST classification](http://yann.lecun.com/exdb/mnist/). [The MNIST database](https://en.wikipedia.org/wiki/MNIST_database) (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used as a first test for new classification algorithms. \n",
        "We follow this tradition to investigate the performance of artificial neural networks of different complexity on MNIST. However, since MNIST is too easy for accessing the full power of modern machine learning algorithms (see e.g. [this post](https://twitter.com/goodfellow_ian/status/852591106655043584)) we will extend our analysis to the recently introduced, harder [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist).\n",
        "\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "- You should have a running installation of [tensorflow](https://www.tensorflow.org/install/) and [keras](https://keras.io/). Feel free to gain inspiration from the [Keras example directory](https://github.com/keras-team/keras/tree/master/examples) for your implementations.\n",
        "- You should know the concepts \"multilayer perceptron\", \"stochastic gradient descent with minibatches\", \"convolutional neural network\", \"training and validation data\", \"overfitting\" and \"early stopping\".\n",
        "\n",
        "### What you will learn\n",
        "\n",
        "- You will learn how to define feedforward neural networks in keras and fit them to data.\n",
        "- You will be guided through a prototyping procedure for the application of deep learning to a specific domain.\n",
        "- You will get in contact with concepts discussed later in the lecture, like \"regularization\", \"batch normalization\" and \"convolutional networks\".\n",
        "- You will gain some experience on the influence of network architecture, optimizer and regularization choices on the goodness of fit.\n",
        "- You will learn to be more patient :) Some fits may take your computer quite a bit of time; run them over night (or on an external server).\n",
        "\n",
        "### Evaluation criteria\n",
        "\n",
        "The evaluation is (mostly) based on the figures you submit and your answer sentences. Provide clear and concise answers respecting the indicated maximum length (answers to the questions should be below the line that says \"Answer to question ...\").\n",
        "\n",
        "**The submitted notebook must be run by you!** We will only do random tests of your code and not re-run the full notebook. There will be fraud detection sessions at the end of the semester.\n",
        "\n",
        "### Your names\n",
        "\n",
        "**Before you start**: please enter your full name(s) in the field below."
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-03-09T09:08:24.514461Z",
          "start_time": "2018-03-09T09:08:24.506410Z"
        },
        "id": "JPStoLUKtss6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "student1 = \"Tomas Giro\"\n",
        "student2 = \"Johan Morin\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-22T21:52:59.697375Z",
          "start_time": "2018-02-22T21:52:59.689443Z"
        },
        "id": "H9FjP0T1tss9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Some helper functions\n",
        "\n",
        "For your convenience we provide here some functions to preprocess the data and plot the results later. Simply run the following cells with `Shift-Enter`.\n",
        "\n",
        "### Dependencies and constants"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-23T14:27:09.352019Z",
          "start_time": "2018-02-23T14:27:08.476310Z"
        },
        "id": "A9fI4O1-tss-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "import json\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, Activation, BatchNormalization\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "weCY44XKtstB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Plotting"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-23T15:11:52.252208Z",
          "start_time": "2018-02-23T15:11:52.121360Z"
        },
        "id": "TrEbSBIctstC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_some_samples(x, y = [], yhat = [], select_from = [], \n",
        "                      ncols = 6, nrows = 4, xdim = 28, ydim = 28,\n",
        "                      label_mapping = range(10)):\n",
        "    \"\"\"plot some input vectors as grayscale images (optionally together with their assigned or predicted labels).\n",
        "    \n",
        "    x is an NxD - dimensional array, where D is the length of an input vector and N is the number of samples.\n",
        "    Out of the N samples, ncols x nrows indices are randomly selected from the list select_from\n",
        "    (if it is empty, select_from becomes range(N)).\n",
        "    \n",
        "    Keyword arguments:\n",
        "    y             -- corresponding labels to plot in green below each image.\n",
        "    yhat          -- corresponding predicted labels to plot in red below each image.\n",
        "    select_from   -- list of indices from which to select the images.\n",
        "    ncols, nrows  -- number of columns and rows to plot.\n",
        "    xdim, ydim    -- number of pixels of the images in x- and y-direction.\n",
        "    label_mapping -- map labels to digits.\n",
        "    \n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(nrows, ncols)\n",
        "    if len(select_from) == 0:\n",
        "        select_from = range(x.shape[0])\n",
        "    indices = np.random.choice(select_from, size = min(ncols * nrows, len(select_from)), replace = False)\n",
        "    for i, ind in enumerate(indices):\n",
        "        thisax = ax[i//ncols,i%ncols]\n",
        "        thisax.matshow(x[ind].reshape(xdim, ydim), cmap='gray')\n",
        "        thisax.set_axis_off()\n",
        "        if len(y) != 0:\n",
        "            j = y[ind] if type(y[ind]) != np.ndarray else y[ind].argmax()\n",
        "            thisax.text(0, 0, (label_mapping[j]), color='green', \n",
        "                                                       verticalalignment='top',\n",
        "                                                       transform=thisax.transAxes)\n",
        "        if len(yhat) != 0:\n",
        "            k = yhat[ind] if type(yhat[ind]) != np.ndarray else yhat[ind].argmax()\n",
        "            thisax.text(1, 0, (label_mapping[k]), color='red',\n",
        "                                             verticalalignment='top',\n",
        "                                             horizontalalignment='right',\n",
        "                                             transform=thisax.transAxes)\n",
        "    return fig\n",
        "\n",
        "def prepare_standardplot(title, xlabel):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.suptitle(title)\n",
        "    ax1.set_ylabel('categorical cross entropy')\n",
        "    ax1.set_xlabel(xlabel)\n",
        "    ax1.set_yscale('log')\n",
        "    ax2.set_ylabel('accuracy [% correct]')\n",
        "    ax2.set_xlabel(xlabel)\n",
        "    return fig, ax1, ax2\n",
        "\n",
        "def finalize_standardplot(fig, ax1, ax2):\n",
        "    ax1handles, ax1labels = ax1.get_legend_handles_labels()\n",
        "    if len(ax1labels) > 0:\n",
        "        ax1.legend(ax1handles, ax1labels)\n",
        "    ax2handles, ax2labels = ax2.get_legend_handles_labels()\n",
        "    if len(ax2labels) > 0:\n",
        "        ax2.legend(ax2handles, ax2labels)\n",
        "    fig.tight_layout()\n",
        "    plt.subplots_adjust(top=0.9)\n",
        "\n",
        "def plot_history(history, title):\n",
        "    fig, ax1, ax2 = prepare_standardplot(title, 'epoch')\n",
        "    ax1.plot(history['loss'], label = \"training\")\n",
        "    ax1.plot(history['val_loss'], label = \"validation\")\n",
        "    ax2.plot(history['acc'], label = \"training\")\n",
        "    ax2.plot(history['val_acc'], label = \"validation\")\n",
        "    finalize_standardplot(fig, ax1, ax2)\n",
        "    return fig\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-23T14:27:44.442862Z",
          "start_time": "2018-02-23T14:27:09.505547Z"
        },
        "id": "ghqK4b7FtstH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "(x_fashion_train, y_fashion_train), (x_fashion_test, y_fashion_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AWcbSKBF07GH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "def save(history,model,code):\n",
        "  name='history'+str(code)+'.json'\n",
        "  with open(name, 'w') as f:\n",
        "    try:\n",
        "      json.dump(history, f)\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      json.dump(history.history,f)\n",
        "    except:\n",
        "      pass\n",
        "  files.download(name)\n",
        "  \n",
        "  \n",
        "  model.save('model'+str(code)+'.h5') \n",
        "  files.download('model'+str(code)+'.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vP_PplpStstE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Data import and visualization (4 points)\n",
        "\n",
        "### Description\n",
        "\n",
        "### Loading the data\n",
        "\n",
        "The datasets we use in this project (MNIST, Fashion-MNIST) consists of grayscale images with 28x28 pixels. Keras comes with a convenient in-built [data importer](https://keras.io/datasets/) for common datasets.\n",
        "\n",
        "1. As a warm-up exercise, use this importer to (down-)load the MNIST and Fashion-MNIST dataset. Assign useful variables to test & train images and labels for both datasets respectively. (2 pts)\n",
        "2. Use the corresponding plotting function defined above to plot some samples of the two datasets. What do the green digits at the bottom left of each image indicate? (1 sentence max.) (2 pts)\n",
        "\n",
        "The low resolution (and grayscale) of the images certainly misses some information that could be helpful for classifying the images. However, since the data has lower dimensionality due to the low resolution, the fitting procedures converge faster. This is an advantage in situations like here (or generally when prototyping), were we want to try many different things without having to wait too long for computations to finish.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "z38f6B_btstF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ]
    },
    {
      "metadata": {
        "id": "-GHkZA3BtstO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Answer to question 1: Done above.\n",
        "\n",
        "\n",
        "Answer to question 2:"
      ]
    },
    {
      "metadata": {
        "id": "lGrDqazKtstP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The green digits under the images indicate the corresponding labels of the images above. (I modified the function for it to work with strings and not only integers.)\n",
        "              \n",
        "             "
      ]
    },
    {
      "metadata": {
        "id": "7VqcRHWdtstQ",
        "colab_type": "code",
        "outputId": "427acb7d-1c40-47af-bf4c-2449ea2aaaff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "plot_some_samples(x=x_train, y=y_train , yhat = [], select_from = [], \n",
        "                      ncols = 6, nrows = 4, xdim = 28, ydim = 28,\n",
        "                      label_mapping = range(10))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAECCAYAAADjBlzIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXtcVV7wAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 24 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAECCAYAAADjBlzIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXtcVV7wAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 24 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "RqDoKeSBtstT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Data pre-processing**: To prepare for fitting we transform the labels to one hot coding, i.e. for 5 classes, label 2 becomes the vector [0, 0, 1, 0, 0] (python uses 0-indexing). Furthermore we reshape (flatten) the input images to input vectors and rescale the data into the range [0,1]."
      ]
    },
    {
      "metadata": {
        "id": "_waFDZpktstY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2])/np.max(x_train)\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1]*x_test.shape[2])/np.max(x_test)\n",
        "\n",
        "x_fashion_train = x_fashion_train.reshape(x_fashion_train.shape[0], x_fashion_train.shape[1]*x_fashion_train.shape[2])/np.max(x_fashion_train)\n",
        "x_fashion_test = x_fashion_test.reshape(x_fashion_test.shape[0], x_fashion_test.shape[1]*x_fashion_test.shape[2])/np.max(x_fashion_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ia-8-umGtstU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train = keras.utils.to_categorical(y_train)\n",
        "y_test = keras.utils.to_categorical(y_test)\n",
        "\n",
        "y_fashion_train = keras.utils.to_categorical(y_fashion_train)\n",
        "y_fashion_test = keras.utils.to_categorical(y_fashion_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r3NlKH3ststM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_tr={}\n",
        "x_tr['numbers']=x_train\n",
        "x_tr['fashion']=x_fashion_train\n",
        "y_tr={}\n",
        "y_tr['numbers']=y_train\n",
        "y_tr['fashion']=y_fashion_train\n",
        "x_te={}\n",
        "x_te['numbers']=x_test\n",
        "x_te['fashion']=x_fashion_test\n",
        "y_te={}\n",
        "y_te['numbers']=y_test\n",
        "y_te['fashion']=y_fashion_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BJ7UQ4hxtste",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: No hidden layer (10 points)\n",
        "\n",
        "### Description\n",
        "\n",
        "Define and fit a model without a hidden layer (since we will use multi-layer models later in this project, you can define a general constructor function for models with an arbitrary number of hidden layers already at this point). (1 pt for each step)\n",
        "\n",
        "1. Use the softmax activation for the output layer.\n",
        "2. Use the categorical_crossentropy loss.\n",
        "3. Add the accuracy metric to the metrics.\n",
        "4. Choose stochastic gradient descent for the optimizer.\n",
        "5. Choose a minibatch size of 128.\n",
        "6. Fit for as many epochs as needed to see no further decrease in the validation loss.\n",
        "7. Plot the output of the fitting procedure (a history object) using the function plot_history defined above.\n",
        "8. Determine the indices of all test images that are misclassified by the fitted model and plot some of them using the function \n",
        "   `plot_some_samples(x_test, y_test, yhat_test, error_indices)`. Explain the green and red digits at the bottom of each image.\n",
        "9. Repeat the above steps for fitting the network to the Fashion-MNIST dataset.\n",
        "\n",
        "\n",
        "Hints:\n",
        "* Read the keras docs, in particular [Getting started with the Keras Sequential model](https://keras.io/getting-started/sequential-model-guide/).\n",
        "* Have a look at the keras [examples](https://github.com/keras-team/keras/tree/master/examples), e.g. [mnist_mlp](https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py)."
      ]
    },
    {
      "metadata": {
        "id": "ltaO4KBntste",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "65xtk_VAtstf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DON'T RUN\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "\n",
        "epochs=300\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(10, input_shape=(784,)),\n",
        "    Activation('softmax'),\n",
        "])\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.01, clipnorm=1.)\n",
        "model.compile(optimizer=sgd,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "# Train the model, iterating on the data in batches of 128 samples\n",
        "history=model.fit(x_train, y_train, epochs=epochs, batch_size=128, validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ysygoG_ltstj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DON'T RUN\n",
        "plot_history(history.history, title='output of the fitting procedure')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TCGs_Slztstm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "error_indices=np.flatnonzero((keras.utils.to_categorical(model.predict(x_test).argmax(1))-y_test).max(1))\n",
        "plot_some_samples(x_test, y_test, model.predict(x_test), error_indices,label_mapping = range(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BTfh8dCftstt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Answer to question 10:"
      ]
    },
    {
      "metadata": {
        "id": "TQR_F6Nctstv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I couldn't find a question 10. For question 8, the green and red numbers correspond to the  corresponding labels of the images above, and the predicted labels. We can observe that many errors of the predicting algorithm are understandable because some fours look like nines, and some sixes look like zeros."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "miL7Sahotstx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DON'T RUN\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "epochs=300\n",
        "\n",
        "model_fashion = Sequential([\n",
        "    Dense(10, input_shape=(784,)),\n",
        "    Activation('softmax')\n",
        "])\n",
        "\n",
        "sgd_fashion = keras.optimizers.SGD(lr=0.01, clipnorm=1.)\n",
        "model_fashion.compile(optimizer=sgd_fashion,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "# Train the model, iterating on the data in batches of 128 samples\n",
        "history_fashion=model_fashion.fit(x_fashion_train, y_fashion_train, epochs=epochs, batch_size=128, validation_data=(x_fashion_test, y_fashion_test))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zQzqO2G6tst2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DON'T RUN\n",
        "plot_history(history.history, title='output of the fitting procedure for fashion dataset')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M7orDInYtst8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "ab9a33a3-1063-4607-e1e8-bb7a7ede95b0"
      },
      "cell_type": "code",
      "source": [
        "error_indices=np.flatnonzero(\n",
        "    (\n",
        "        keras.utils.to_categorical( model_fashion.predict(x_fashion_test).argmax(1) )  - y_fashion_test\n",
        "    ).max(1)\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "label_mapping=['T-sh'\n",
        ",'Trou'\n",
        ",'Pull'\n",
        ",'Dres'\n",
        ",'Coat'\n",
        ",'San'\n",
        ",'Shir'\n",
        ",'Snea'\n",
        ",'Bag'\n",
        ",'Boot']\n",
        "\n",
        "plot_some_samples(x_fashion_test, y_fashion_test, model_fashion.predict(x_fashion_test), error_indices,label_mapping = label_mapping)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-0e3e1f2fc2f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m error_indices=np.flatnonzero(\n\u001b[1;32m      2\u001b[0m     (\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel_fashion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_fashion_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m  \u001b[0;34m-\u001b[0m \u001b[0my_fashion_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     ).max(1)\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_fashion' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "C02TOwEhtst-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: One hidden layer, different optizimizers & overfitting (10 points)\n",
        "\n",
        "### Description\n",
        "\n",
        "Train a network with one hidden layer and compare different optimizers.\n",
        "\n",
        "1. Use one hidden layer with 128 units and the 'relu' activation. Use the [summary method](https://keras.io/models/about-keras-models/) to display your model in a compact way. (1 pt)\n",
        "2. Fit the model for 50 epochs with different learning rates of stochastic gradient descent (SGD). (1pt)\n",
        "3. Replace the stochastic gradient descent optimizer with the [Adam optimizer](https://keras.io/optimizers/#adam). (1pt)\n",
        "4. Plot the learning curves of SGD with a reasonable learning rate (i.e. in the range [0.01,0.1]) together with the learning curves of Adam in the same figure. Take care of a reasonable labeling of the curves in the plot. (2pts)\n",
        "5. Answer the questions below. (4pts)\n",
        "6. Run the network (using the Adam optimizer) on the Fashion-MNIST dataset and plot the learning curves using the plot_history function defined above. (1pt)"
      ]
    },
    {
      "metadata": {
        "id": "kXCum03Ptst_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Solution\n",
        "! attention: very long execution !"
      ]
    },
    {
      "metadata": {
        "id": "Qj6ykYYac1fF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "############## 3.1 ###############\n",
        "\n",
        "model3 = Sequential([\n",
        "    Dense(128, input_shape=(784,)),\n",
        "    Activation('relu'),\n",
        "    Dense(10),\n",
        "    Activation('softmax'),\n",
        "])\n",
        "\n",
        "model3.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "_F6gLAqqtsuA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "################ 3.2 ###############\n",
        "\n",
        "lr=[0.01, 0.03, 0.09]\n",
        "historys={'adam':[],'sgd':[]}\n",
        "epochs=50 # must be 50 by requirement\n",
        "\n",
        "\n",
        "for i in range(len(lr)):\n",
        "  ##### delete\n",
        "    pass\n",
        "    sgd3 = keras.optimizers.SGD(lr=lr[i], clipnorm=1.)\n",
        "    model3.compile(optimizer=sgd3,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['acc'])\n",
        "\n",
        "    historys['sgd']+=[model3.fit(x_train, y_train, epochs=epochs, batch_size=128, validation_data=(x_test, y_test))]\n",
        "\n",
        "    \n",
        "model3 = Sequential([\n",
        "    Dense(128, input_shape=(784,)),\n",
        "    Activation('relu'),\n",
        "    Dense(10),\n",
        "    Activation('softmax'),\n",
        "])\n",
        "\n",
        "    \n",
        "\n",
        "for i in range(len(lr)):\n",
        "    adam3 = keras.optimizers.Adam(lr=lr[i], beta_1=0.9, beta_2=0.999)\n",
        "    model3.compile(optimizer=adam3,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['acc'])\n",
        "\n",
        "    historys['adam']+=[model3.fit(x_train, y_train, epochs=epochs, batch_size=128, validation_data=(x_test, y_test))]\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VXlJHgr4Dt0x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# This plotting routine might help you ...\n",
        "def comparison_plot(history_sgd, history_adam, label1, label2, title):\n",
        "    fig, ax1, ax2 = prepare_standardplot(title, \"epochs\")\n",
        "    ax1.plot(history_sgd.history['loss'], label=label1 + ' training')\n",
        "    ax1.plot(history_sgd.history['val_loss'], label=label1 + ' validation')\n",
        "    ax1.plot(history_adam.history['loss'], label=label2 + ' training')\n",
        "    ax1.plot(history_adam.history['val_loss'], label=label2 + ' validation')\n",
        "    ax2.plot(history_sgd.history['acc'], label=label1 + ' training')\n",
        "    ax2.plot(history_sgd.history['val_acc'], label=label1 + ' validation')\n",
        "    ax2.plot(history_adam.history['acc'], label=label2 + ' training')\n",
        "    ax2.plot(history_adam.history['val_acc'], label=label2 + ' validation')\n",
        "    finalize_standardplot(fig, ax1, ax2)\n",
        "    return fig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pA-sdeMOtsuF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### Do not execute !\n",
        "for i in range(len(lr)):\n",
        "    comparison_plot(historys['sgd'][i], historys['adam'][i], 'sgd', 'adam', 'Numbers: fitting procedure with lr='+str(lr[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ycTeuRM-tsuN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##################### 3.5 ################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TG39EgIrtsuQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Question**: What happens if the learning rate of SGD is A) very large B) very small? Please answer A) and B) with one full sentence each (double click this markdown cell to edit).\n",
        "\n",
        "**Answer**:\n",
        "\n",
        "A) The gradient descent can overshoot the minimum, it might fail to converge or diverge.\n",
        "\n",
        "B) The gradient descent will converge so slowly to the minimum, that it might take too much computational time.\n",
        "\n",
        "**Question**: At which epoch (approximately) does the Adam optimizer start to overfit (on MNIST)? Please answer with one full sentence.\n",
        "\n",
        "**Answer**:\n",
        "\n",
        "**Question**: Explain the qualitative difference between the loss curves and the accuracy curves with respect to signs of overfitting. Please answer with at most 3 full sentences.\n",
        "\n",
        "**Answer**: The sign of overfitting for the loss curve is when the decreasing loss finds it's minimum and starts increasing.\n",
        "The sign of overfitting for the accuracy curve is when the increasing accuracy finds it's maximum and starts decreasing."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "9Et6xkemtsuR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "######################### 3.6 ####################\n",
        "\n",
        "model3 = Sequential([\n",
        "    Dense(128, input_shape=(784,)),\n",
        "    Activation('relu'),\n",
        "    Dense(10),\n",
        "    Activation('softmax'),\n",
        "])\n",
        "\n",
        "### parameters\n",
        "lr=[0.01, 0.03, 0.09]\n",
        "historys={'adam':[],'sgd':[]}\n",
        "epochs=3\n",
        "\n",
        "\n",
        "for i in range(len(lr)):\n",
        "    sgd3 = keras.optimizers.SGD(lr=lr[i], clipnorm=1.)\n",
        "    model3.compile(optimizer=sgd3,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['acc'])\n",
        "\n",
        "    historys['sgd']+=[model3.fit(x_fashion_train, y_fashion_train, epochs=epochs, batch_size=128, validation_data=(x_fashion_test, y_fashion_test))]\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(lr)):\n",
        "    adam3 = keras.optimizers.Adam(lr=lr[i], beta_1=0.9, beta_2=0.999)\n",
        "    model3.compile(optimizer=adam3,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['acc'])\n",
        "\n",
        "    historys['adam']+=[model3.fit(x_fashion_train, y_fashion_train, epochs=epochs, batch_size=128, validation_data=(x_fashion_test, y_fashion_test))]\n",
        "\n",
        "\n",
        "# This plotting routine might help you ...\n",
        "def comparison_plot(history_sgd, history_adam, label1, label2, title):\n",
        "    fig, ax1, ax2 = prepare_standardplot(title, \"epochs\")\n",
        "    ax1.plot(history_sgd.history['loss'], label=label1 + ' training')\n",
        "    ax1.plot(history_sgd.history['val_loss'], label=label1 + ' validation')\n",
        "    ax1.plot(history_adam.history['loss'], label=label2 + ' training')\n",
        "    ax1.plot(history_adam.history['val_loss'], label=label2 + ' validation')\n",
        "    ax2.plot(history_sgd.history['acc'], label=label1 + ' training')\n",
        "    ax2.plot(history_sgd.history['val_acc'], label=label1 + ' validation')\n",
        "    ax2.plot(history_adam.history['acc'], label=label2 + ' training')\n",
        "    ax2.plot(history_adam.history['val_acc'], label=label2 + ' validation')\n",
        "    finalize_standardplot(fig, ax1, ax2)\n",
        "    return fig\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9H6Axt0CtsuT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(len(lr)):\n",
        "    comparison_plot(historys['sgd'], historys['adam'], 'sgd', 'adam', 'Fashion: fitting procedure with lr='+str(lr[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h79KTvbStsuW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 4: Model performance as a function of number of hidden neurons (8 points)\n",
        "\n",
        "### Description\n",
        "\n",
        "Investigate how the best validation loss and accuracy depends on the number of hidden neurons in a single layer.\n",
        "\n",
        "1. Fit a reasonable number of models (e.g. 5) with different hidden layer sizes (between 10 and 1000 hidden neurons) to the MNIST dataset. You may use the Adam optimizer and a meaningful number of epochs (overfitting!). (3 pts)\n",
        "2. Plot the best validation loss and accuracy versus the number of hidden neurons. Is the observed trend in accordance with the [general approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)? If not, what might be practical reasons for the deviation? (2 sentences max.) (3 pts)\n",
        "3. Repeat steps 1. & 2. for the Fashion-MNIST dataset. (2 pts)\n",
        "\n",
        "In this exercise we fit each model only for one initialization and random seed. In practice one would collect some statistics (e.g. 25-, 50-, 75-percentiles) for each layer size by fitting each model several times with different initializations and the random seeds. You may also want to do this here. It is a good exercise, but not mandatory as it takes quite a bit of computation time.\n",
        "\n",
        "### Solution"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-23T14:58:15.181352Z",
          "start_time": "2018-02-23T14:31:52.623267Z"
        },
        "scrolled": true,
        "id": "zcNltZjttsuX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 20451
        },
        "outputId": "b788cdd3-4d4b-4cb5-b90b-03e472e77e0f"
      },
      "cell_type": "code",
      "source": [
        "# computation of the data\n",
        "DATA={}\n",
        "for dataset in ['numbers', 'fashion']:\n",
        "    \n",
        "    data={}\n",
        "    data['hidden neurons']=[10, 300, 600, 800, 1000]\n",
        "    data['history']=[]\n",
        "    data['epochs']={'numbers':[500, 20, 20, 20, 20],\n",
        "                    'fashion':[500, 25, 15, 15, 15] } # <----- change the epochs here !\n",
        "    data['best validation loss']=[]\n",
        "    data['best validation accuracy']=[]\n",
        "\n",
        "    \n",
        "    for i in range(len(data['hidden neurons'])):\n",
        "        model4= Sequential([\n",
        "            Dense(data['hidden neurons'][i], input_shape=(784,)),\n",
        "            Activation('relu'),\n",
        "            Dense(10),\n",
        "            Activation('softmax'),\n",
        "        ])\n",
        "\n",
        "        model4.summary()\n",
        "\n",
        "        adam3 = keras.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
        "        model4.compile(optimizer=adam3,\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['acc'])\n",
        "        \n",
        "        h=model4.fit(x_tr[dataset], y_tr[dataset], epochs=data['epochs'][dataset][i],\n",
        "                                     batch_size=128, validation_data=(x_te[dataset], y_te[dataset]))\n",
        "\n",
        "        data['history'].append(h.history)\n",
        "        data['best validation loss']+=[ np.array(data['history'][-1]['val_loss']).min()]\n",
        "        data['best validation accuracy']+=[ np.array(data['history'][-1]['val_acc']).max()]\n",
        "\n",
        "    DATA[dataset]=data\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_175 (Dense)            (None, 10)                7850      \n",
            "_________________________________________________________________\n",
            "activation_175 (Activation)  (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_176 (Dense)            (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "activation_176 (Activation)  (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 7,960\n",
            "Trainable params: 7,960\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/500\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 1.0621 - acc: 0.7059 - val_loss: 0.4977 - val_acc: 0.8745\n",
            "Epoch 2/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.4246 - acc: 0.8870 - val_loss: 0.3588 - val_acc: 0.9022\n",
            "Epoch 3/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.3447 - acc: 0.9038 - val_loss: 0.3156 - val_acc: 0.9113\n",
            "Epoch 4/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.3131 - acc: 0.9128 - val_loss: 0.2971 - val_acc: 0.9157\n",
            "Epoch 5/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.2953 - acc: 0.9176 - val_loss: 0.2882 - val_acc: 0.9189\n",
            "Epoch 6/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.2833 - acc: 0.9211 - val_loss: 0.2773 - val_acc: 0.9243\n",
            "Epoch 7/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.2745 - acc: 0.9234 - val_loss: 0.2708 - val_acc: 0.9267\n",
            "Epoch 8/500\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.2672 - acc: 0.9252 - val_loss: 0.2680 - val_acc: 0.9257\n",
            "Epoch 9/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.2621 - acc: 0.9271 - val_loss: 0.2641 - val_acc: 0.9271\n",
            "Epoch 10/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.2570 - acc: 0.9285 - val_loss: 0.2601 - val_acc: 0.9284\n",
            "Epoch 11/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.2528 - acc: 0.9296 - val_loss: 0.2570 - val_acc: 0.9300\n",
            "Epoch 12/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.2494 - acc: 0.9307 - val_loss: 0.2537 - val_acc: 0.9303\n",
            "Epoch 13/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.2459 - acc: 0.9310 - val_loss: 0.2524 - val_acc: 0.9313\n",
            "Epoch 14/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.2428 - acc: 0.9326 - val_loss: 0.2508 - val_acc: 0.9309\n",
            "Epoch 15/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.2401 - acc: 0.9328 - val_loss: 0.2500 - val_acc: 0.9308\n",
            "Epoch 16/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.2374 - acc: 0.9335 - val_loss: 0.2488 - val_acc: 0.9321\n",
            "Epoch 17/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.2350 - acc: 0.9349 - val_loss: 0.2475 - val_acc: 0.9317\n",
            "Epoch 18/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.2327 - acc: 0.9348 - val_loss: 0.2459 - val_acc: 0.9329\n",
            "Epoch 19/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.2305 - acc: 0.9352 - val_loss: 0.2468 - val_acc: 0.9306\n",
            "Epoch 20/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.2286 - acc: 0.9359 - val_loss: 0.2435 - val_acc: 0.9323\n",
            "Epoch 21/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.2266 - acc: 0.9367 - val_loss: 0.2435 - val_acc: 0.9326\n",
            "Epoch 22/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.2250 - acc: 0.9373 - val_loss: 0.2449 - val_acc: 0.9321\n",
            "Epoch 23/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.2234 - acc: 0.9383 - val_loss: 0.2414 - val_acc: 0.9348\n",
            "Epoch 24/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.2217 - acc: 0.9381 - val_loss: 0.2418 - val_acc: 0.9323\n",
            "Epoch 25/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.2202 - acc: 0.9382 - val_loss: 0.2406 - val_acc: 0.9342\n",
            "Epoch 26/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.2186 - acc: 0.9394 - val_loss: 0.2400 - val_acc: 0.9331\n",
            "Epoch 27/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.2174 - acc: 0.9392 - val_loss: 0.2409 - val_acc: 0.9330\n",
            "Epoch 28/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.2160 - acc: 0.9395 - val_loss: 0.2400 - val_acc: 0.9335\n",
            "Epoch 29/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.2145 - acc: 0.9408 - val_loss: 0.2395 - val_acc: 0.9340\n",
            "Epoch 30/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.2131 - acc: 0.9406 - val_loss: 0.2390 - val_acc: 0.9330\n",
            "Epoch 31/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.2118 - acc: 0.9409 - val_loss: 0.2360 - val_acc: 0.9349\n",
            "Epoch 32/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.2107 - acc: 0.9419 - val_loss: 0.2371 - val_acc: 0.9335\n",
            "Epoch 33/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.2095 - acc: 0.9417 - val_loss: 0.2387 - val_acc: 0.9337\n",
            "Epoch 34/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.2083 - acc: 0.9425 - val_loss: 0.2393 - val_acc: 0.9349\n",
            "Epoch 35/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.2070 - acc: 0.9422 - val_loss: 0.2372 - val_acc: 0.9345\n",
            "Epoch 36/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.2056 - acc: 0.9423 - val_loss: 0.2370 - val_acc: 0.9353\n",
            "Epoch 37/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.2048 - acc: 0.9428 - val_loss: 0.2363 - val_acc: 0.9358\n",
            "Epoch 38/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.2035 - acc: 0.9429 - val_loss: 0.2382 - val_acc: 0.9336\n",
            "Epoch 39/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.2023 - acc: 0.9430 - val_loss: 0.2351 - val_acc: 0.9357\n",
            "Epoch 40/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.2012 - acc: 0.9441 - val_loss: 0.2345 - val_acc: 0.9369\n",
            "Epoch 41/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.2001 - acc: 0.9438 - val_loss: 0.2334 - val_acc: 0.9366\n",
            "Epoch 42/500\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.1991 - acc: 0.9442 - val_loss: 0.2330 - val_acc: 0.9363\n",
            "Epoch 43/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1977 - acc: 0.9446 - val_loss: 0.2347 - val_acc: 0.9344\n",
            "Epoch 44/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1965 - acc: 0.9453 - val_loss: 0.2327 - val_acc: 0.9363\n",
            "Epoch 45/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1952 - acc: 0.9458 - val_loss: 0.2315 - val_acc: 0.9385\n",
            "Epoch 46/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1934 - acc: 0.9459 - val_loss: 0.2308 - val_acc: 0.9379\n",
            "Epoch 47/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1922 - acc: 0.9462 - val_loss: 0.2303 - val_acc: 0.9369\n",
            "Epoch 48/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1909 - acc: 0.9466 - val_loss: 0.2297 - val_acc: 0.9372\n",
            "Epoch 49/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1898 - acc: 0.9471 - val_loss: 0.2297 - val_acc: 0.9373\n",
            "Epoch 50/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1885 - acc: 0.9476 - val_loss: 0.2270 - val_acc: 0.9377\n",
            "Epoch 51/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1870 - acc: 0.9479 - val_loss: 0.2270 - val_acc: 0.9389\n",
            "Epoch 52/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1863 - acc: 0.9477 - val_loss: 0.2257 - val_acc: 0.9382\n",
            "Epoch 53/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1851 - acc: 0.9487 - val_loss: 0.2272 - val_acc: 0.9384\n",
            "Epoch 54/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1839 - acc: 0.9488 - val_loss: 0.2233 - val_acc: 0.9391\n",
            "Epoch 55/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1830 - acc: 0.9491 - val_loss: 0.2250 - val_acc: 0.9384\n",
            "Epoch 56/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1816 - acc: 0.9497 - val_loss: 0.2245 - val_acc: 0.9385\n",
            "Epoch 57/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1805 - acc: 0.9498 - val_loss: 0.2233 - val_acc: 0.9391\n",
            "Epoch 58/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1799 - acc: 0.9500 - val_loss: 0.2236 - val_acc: 0.9374\n",
            "Epoch 59/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1789 - acc: 0.9505 - val_loss: 0.2242 - val_acc: 0.9397\n",
            "Epoch 60/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1781 - acc: 0.9499 - val_loss: 0.2218 - val_acc: 0.9395\n",
            "Epoch 61/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1768 - acc: 0.9504 - val_loss: 0.2256 - val_acc: 0.9389\n",
            "Epoch 62/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1766 - acc: 0.9508 - val_loss: 0.2229 - val_acc: 0.9402\n",
            "Epoch 63/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1756 - acc: 0.9511 - val_loss: 0.2227 - val_acc: 0.9389\n",
            "Epoch 64/500\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.1748 - acc: 0.9508 - val_loss: 0.2211 - val_acc: 0.9400\n",
            "Epoch 65/500\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 0.1739 - acc: 0.9512 - val_loss: 0.2202 - val_acc: 0.9401\n",
            "Epoch 66/500\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.1732 - acc: 0.9516 - val_loss: 0.2227 - val_acc: 0.9384\n",
            "Epoch 67/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1725 - acc: 0.9519 - val_loss: 0.2222 - val_acc: 0.9402\n",
            "Epoch 68/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1716 - acc: 0.9514 - val_loss: 0.2234 - val_acc: 0.9401\n",
            "Epoch 69/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1709 - acc: 0.9518 - val_loss: 0.2216 - val_acc: 0.9395\n",
            "Epoch 70/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1701 - acc: 0.9522 - val_loss: 0.2224 - val_acc: 0.9395\n",
            "Epoch 71/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1700 - acc: 0.9523 - val_loss: 0.2219 - val_acc: 0.9403\n",
            "Epoch 72/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1692 - acc: 0.9525 - val_loss: 0.2218 - val_acc: 0.9388\n",
            "Epoch 73/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1683 - acc: 0.9525 - val_loss: 0.2214 - val_acc: 0.9405\n",
            "Epoch 74/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1678 - acc: 0.9525 - val_loss: 0.2219 - val_acc: 0.9408\n",
            "Epoch 75/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1672 - acc: 0.9529 - val_loss: 0.2236 - val_acc: 0.9401\n",
            "Epoch 76/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1665 - acc: 0.9528 - val_loss: 0.2225 - val_acc: 0.9399\n",
            "Epoch 77/500\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.1659 - acc: 0.9533 - val_loss: 0.2208 - val_acc: 0.9396\n",
            "Epoch 78/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1652 - acc: 0.9538 - val_loss: 0.2210 - val_acc: 0.9403\n",
            "Epoch 79/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1648 - acc: 0.9537 - val_loss: 0.2237 - val_acc: 0.9415\n",
            "Epoch 80/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1639 - acc: 0.9544 - val_loss: 0.2204 - val_acc: 0.9416\n",
            "Epoch 81/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1638 - acc: 0.9539 - val_loss: 0.2211 - val_acc: 0.9411\n",
            "Epoch 82/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1631 - acc: 0.9543 - val_loss: 0.2211 - val_acc: 0.9407\n",
            "Epoch 83/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1625 - acc: 0.9542 - val_loss: 0.2203 - val_acc: 0.9407\n",
            "Epoch 84/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1619 - acc: 0.9541 - val_loss: 0.2198 - val_acc: 0.9414\n",
            "Epoch 85/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1614 - acc: 0.9549 - val_loss: 0.2210 - val_acc: 0.9404\n",
            "Epoch 86/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1609 - acc: 0.9550 - val_loss: 0.2232 - val_acc: 0.9412\n",
            "Epoch 87/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1606 - acc: 0.9548 - val_loss: 0.2207 - val_acc: 0.9406\n",
            "Epoch 88/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1599 - acc: 0.9552 - val_loss: 0.2221 - val_acc: 0.9419\n",
            "Epoch 89/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1590 - acc: 0.9555 - val_loss: 0.2215 - val_acc: 0.9410\n",
            "Epoch 90/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1590 - acc: 0.9549 - val_loss: 0.2218 - val_acc: 0.9413\n",
            "Epoch 91/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1582 - acc: 0.9556 - val_loss: 0.2217 - val_acc: 0.9403\n",
            "Epoch 92/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1579 - acc: 0.9551 - val_loss: 0.2214 - val_acc: 0.9417\n",
            "Epoch 93/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1572 - acc: 0.9560 - val_loss: 0.2218 - val_acc: 0.9423\n",
            "Epoch 94/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.1569 - acc: 0.9557 - val_loss: 0.2228 - val_acc: 0.9423\n",
            "Epoch 95/500\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 0.1564 - acc: 0.9559 - val_loss: 0.2201 - val_acc: 0.9427\n",
            "Epoch 96/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1558 - acc: 0.9562 - val_loss: 0.2227 - val_acc: 0.9416\n",
            "Epoch 97/500\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 0.1554 - acc: 0.9562 - val_loss: 0.2200 - val_acc: 0.9424\n",
            "Epoch 98/500\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 0.1551 - acc: 0.9564 - val_loss: 0.2200 - val_acc: 0.9417\n",
            "Epoch 99/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1547 - acc: 0.9561 - val_loss: 0.2200 - val_acc: 0.9419\n",
            "Epoch 100/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1542 - acc: 0.9567 - val_loss: 0.2209 - val_acc: 0.9424\n",
            "Epoch 101/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1537 - acc: 0.9571 - val_loss: 0.2212 - val_acc: 0.9422\n",
            "Epoch 102/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1533 - acc: 0.9568 - val_loss: 0.2238 - val_acc: 0.9426\n",
            "Epoch 103/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1526 - acc: 0.9571 - val_loss: 0.2199 - val_acc: 0.9423\n",
            "Epoch 104/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1525 - acc: 0.9565 - val_loss: 0.2225 - val_acc: 0.9419\n",
            "Epoch 105/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1520 - acc: 0.9574 - val_loss: 0.2209 - val_acc: 0.9422\n",
            "Epoch 106/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1517 - acc: 0.9570 - val_loss: 0.2219 - val_acc: 0.9400\n",
            "Epoch 107/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1510 - acc: 0.9571 - val_loss: 0.2205 - val_acc: 0.9420\n",
            "Epoch 108/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1507 - acc: 0.9573 - val_loss: 0.2212 - val_acc: 0.9407\n",
            "Epoch 109/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.1502 - acc: 0.9576 - val_loss: 0.2223 - val_acc: 0.9410\n",
            "Epoch 110/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1502 - acc: 0.9576 - val_loss: 0.2210 - val_acc: 0.9416\n",
            "Epoch 111/500\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.1494 - acc: 0.9579 - val_loss: 0.2211 - val_acc: 0.9415\n",
            "Epoch 112/500\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.1495 - acc: 0.9577 - val_loss: 0.2199 - val_acc: 0.9423\n",
            "Epoch 113/500\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.1490 - acc: 0.9578 - val_loss: 0.2229 - val_acc: 0.9413\n",
            "Epoch 114/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1483 - acc: 0.9582 - val_loss: 0.2229 - val_acc: 0.9422\n",
            "Epoch 115/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1482 - acc: 0.9580 - val_loss: 0.2204 - val_acc: 0.9420\n",
            "Epoch 116/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1478 - acc: 0.9581 - val_loss: 0.2207 - val_acc: 0.9432\n",
            "Epoch 117/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1475 - acc: 0.9582 - val_loss: 0.2224 - val_acc: 0.9424\n",
            "Epoch 118/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.1473 - acc: 0.9577 - val_loss: 0.2229 - val_acc: 0.9431\n",
            "Epoch 119/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1468 - acc: 0.9578 - val_loss: 0.2203 - val_acc: 0.9428\n",
            "Epoch 120/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1465 - acc: 0.9583 - val_loss: 0.2239 - val_acc: 0.9414\n",
            "Epoch 121/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1462 - acc: 0.9577 - val_loss: 0.2223 - val_acc: 0.9426\n",
            "Epoch 122/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1461 - acc: 0.9583 - val_loss: 0.2210 - val_acc: 0.9415\n",
            "Epoch 123/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1453 - acc: 0.9587 - val_loss: 0.2227 - val_acc: 0.9419\n",
            "Epoch 124/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1453 - acc: 0.9587 - val_loss: 0.2224 - val_acc: 0.9419\n",
            "Epoch 125/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1452 - acc: 0.9585 - val_loss: 0.2208 - val_acc: 0.9426\n",
            "Epoch 126/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1447 - acc: 0.9587 - val_loss: 0.2212 - val_acc: 0.9426\n",
            "Epoch 127/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1448 - acc: 0.9590 - val_loss: 0.2213 - val_acc: 0.9427\n",
            "Epoch 128/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1443 - acc: 0.9591 - val_loss: 0.2234 - val_acc: 0.9425\n",
            "Epoch 129/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1434 - acc: 0.9591 - val_loss: 0.2259 - val_acc: 0.9413\n",
            "Epoch 130/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1438 - acc: 0.9588 - val_loss: 0.2226 - val_acc: 0.9428\n",
            "Epoch 131/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1434 - acc: 0.9592 - val_loss: 0.2256 - val_acc: 0.9433\n",
            "Epoch 132/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1427 - acc: 0.9595 - val_loss: 0.2232 - val_acc: 0.9422\n",
            "Epoch 133/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1425 - acc: 0.9590 - val_loss: 0.2222 - val_acc: 0.9448\n",
            "Epoch 134/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1426 - acc: 0.9590 - val_loss: 0.2251 - val_acc: 0.9425\n",
            "Epoch 135/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1422 - acc: 0.9590 - val_loss: 0.2223 - val_acc: 0.9427\n",
            "Epoch 136/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1418 - acc: 0.9592 - val_loss: 0.2230 - val_acc: 0.9429\n",
            "Epoch 137/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1417 - acc: 0.9595 - val_loss: 0.2218 - val_acc: 0.9441\n",
            "Epoch 138/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1414 - acc: 0.9593 - val_loss: 0.2226 - val_acc: 0.9431\n",
            "Epoch 139/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1413 - acc: 0.9592 - val_loss: 0.2235 - val_acc: 0.9433\n",
            "Epoch 140/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1406 - acc: 0.9597 - val_loss: 0.2244 - val_acc: 0.9429\n",
            "Epoch 141/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1405 - acc: 0.9593 - val_loss: 0.2237 - val_acc: 0.9437\n",
            "Epoch 142/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1405 - acc: 0.9604 - val_loss: 0.2232 - val_acc: 0.9441\n",
            "Epoch 143/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1402 - acc: 0.9596 - val_loss: 0.2218 - val_acc: 0.9442\n",
            "Epoch 144/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1394 - acc: 0.9603 - val_loss: 0.2245 - val_acc: 0.9428\n",
            "Epoch 145/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1397 - acc: 0.9598 - val_loss: 0.2242 - val_acc: 0.9433\n",
            "Epoch 146/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1397 - acc: 0.9601 - val_loss: 0.2234 - val_acc: 0.9437\n",
            "Epoch 147/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1392 - acc: 0.9602 - val_loss: 0.2241 - val_acc: 0.9431\n",
            "Epoch 148/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1393 - acc: 0.9602 - val_loss: 0.2229 - val_acc: 0.9435\n",
            "Epoch 149/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1389 - acc: 0.9605 - val_loss: 0.2214 - val_acc: 0.9441\n",
            "Epoch 150/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1385 - acc: 0.9605 - val_loss: 0.2262 - val_acc: 0.9429\n",
            "Epoch 151/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1384 - acc: 0.9599 - val_loss: 0.2247 - val_acc: 0.9440\n",
            "Epoch 152/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.1381 - acc: 0.9603 - val_loss: 0.2243 - val_acc: 0.9426\n",
            "Epoch 153/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1380 - acc: 0.9605 - val_loss: 0.2225 - val_acc: 0.9442\n",
            "Epoch 154/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1375 - acc: 0.9609 - val_loss: 0.2239 - val_acc: 0.9439\n",
            "Epoch 155/500\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.1375 - acc: 0.9605 - val_loss: 0.2236 - val_acc: 0.9437\n",
            "Epoch 156/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1375 - acc: 0.9605 - val_loss: 0.2267 - val_acc: 0.9442\n",
            "Epoch 157/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1370 - acc: 0.9610 - val_loss: 0.2250 - val_acc: 0.9445\n",
            "Epoch 158/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1370 - acc: 0.9608 - val_loss: 0.2248 - val_acc: 0.9437\n",
            "Epoch 159/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1370 - acc: 0.9609 - val_loss: 0.2279 - val_acc: 0.9426\n",
            "Epoch 160/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1368 - acc: 0.9610 - val_loss: 0.2247 - val_acc: 0.9444\n",
            "Epoch 161/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1364 - acc: 0.9606 - val_loss: 0.2242 - val_acc: 0.9450\n",
            "Epoch 162/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1360 - acc: 0.9609 - val_loss: 0.2240 - val_acc: 0.9448\n",
            "Epoch 163/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1360 - acc: 0.9609 - val_loss: 0.2264 - val_acc: 0.9452\n",
            "Epoch 164/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1358 - acc: 0.9609 - val_loss: 0.2266 - val_acc: 0.9436\n",
            "Epoch 165/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1355 - acc: 0.9611 - val_loss: 0.2257 - val_acc: 0.9437\n",
            "Epoch 166/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1358 - acc: 0.9613 - val_loss: 0.2246 - val_acc: 0.9444\n",
            "Epoch 167/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1354 - acc: 0.9612 - val_loss: 0.2253 - val_acc: 0.9439\n",
            "Epoch 168/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1352 - acc: 0.9613 - val_loss: 0.2286 - val_acc: 0.9427\n",
            "Epoch 169/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1351 - acc: 0.9619 - val_loss: 0.2260 - val_acc: 0.9441\n",
            "Epoch 170/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1350 - acc: 0.9612 - val_loss: 0.2271 - val_acc: 0.9432\n",
            "Epoch 171/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1345 - acc: 0.9617 - val_loss: 0.2293 - val_acc: 0.9442\n",
            "Epoch 172/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1344 - acc: 0.9616 - val_loss: 0.2276 - val_acc: 0.9444\n",
            "Epoch 173/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1344 - acc: 0.9612 - val_loss: 0.2262 - val_acc: 0.9445\n",
            "Epoch 174/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1340 - acc: 0.9616 - val_loss: 0.2262 - val_acc: 0.9448\n",
            "Epoch 175/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1338 - acc: 0.9620 - val_loss: 0.2250 - val_acc: 0.9442\n",
            "Epoch 176/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1339 - acc: 0.9617 - val_loss: 0.2274 - val_acc: 0.9442\n",
            "Epoch 177/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1339 - acc: 0.9622 - val_loss: 0.2250 - val_acc: 0.9447\n",
            "Epoch 178/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1336 - acc: 0.9622 - val_loss: 0.2270 - val_acc: 0.9443\n",
            "Epoch 179/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1333 - acc: 0.9616 - val_loss: 0.2295 - val_acc: 0.9432\n",
            "Epoch 180/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1335 - acc: 0.9615 - val_loss: 0.2284 - val_acc: 0.9433\n",
            "Epoch 181/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1331 - acc: 0.9617 - val_loss: 0.2268 - val_acc: 0.9442\n",
            "Epoch 182/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1329 - acc: 0.9619 - val_loss: 0.2284 - val_acc: 0.9433\n",
            "Epoch 183/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1327 - acc: 0.9623 - val_loss: 0.2254 - val_acc: 0.9443\n",
            "Epoch 184/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1325 - acc: 0.9620 - val_loss: 0.2287 - val_acc: 0.9441\n",
            "Epoch 185/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1324 - acc: 0.9619 - val_loss: 0.2283 - val_acc: 0.9441\n",
            "Epoch 186/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1318 - acc: 0.9627 - val_loss: 0.2286 - val_acc: 0.9435\n",
            "Epoch 187/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1323 - acc: 0.9625 - val_loss: 0.2299 - val_acc: 0.9438\n",
            "Epoch 188/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1322 - acc: 0.9624 - val_loss: 0.2302 - val_acc: 0.9436\n",
            "Epoch 189/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1321 - acc: 0.9619 - val_loss: 0.2287 - val_acc: 0.9443\n",
            "Epoch 190/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1319 - acc: 0.9617 - val_loss: 0.2278 - val_acc: 0.9439\n",
            "Epoch 191/500\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.1313 - acc: 0.9621 - val_loss: 0.2292 - val_acc: 0.9448\n",
            "Epoch 192/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1315 - acc: 0.9624 - val_loss: 0.2292 - val_acc: 0.9444\n",
            "Epoch 193/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1311 - acc: 0.9626 - val_loss: 0.2289 - val_acc: 0.9433\n",
            "Epoch 194/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1312 - acc: 0.9622 - val_loss: 0.2318 - val_acc: 0.9429\n",
            "Epoch 195/500\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.1311 - acc: 0.9625 - val_loss: 0.2291 - val_acc: 0.9430\n",
            "Epoch 196/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1310 - acc: 0.9621 - val_loss: 0.2320 - val_acc: 0.9433\n",
            "Epoch 197/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1306 - acc: 0.9623 - val_loss: 0.2310 - val_acc: 0.9437\n",
            "Epoch 198/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1306 - acc: 0.9626 - val_loss: 0.2304 - val_acc: 0.9437\n",
            "Epoch 199/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1303 - acc: 0.9625 - val_loss: 0.2318 - val_acc: 0.9441\n",
            "Epoch 200/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1307 - acc: 0.9626 - val_loss: 0.2306 - val_acc: 0.9442\n",
            "Epoch 201/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1305 - acc: 0.9625 - val_loss: 0.2303 - val_acc: 0.9444\n",
            "Epoch 202/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1302 - acc: 0.9624 - val_loss: 0.2298 - val_acc: 0.9451\n",
            "Epoch 203/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1302 - acc: 0.9627 - val_loss: 0.2312 - val_acc: 0.9443\n",
            "Epoch 204/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1298 - acc: 0.9631 - val_loss: 0.2306 - val_acc: 0.9432\n",
            "Epoch 205/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1299 - acc: 0.9628 - val_loss: 0.2323 - val_acc: 0.9440\n",
            "Epoch 206/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1300 - acc: 0.9629 - val_loss: 0.2317 - val_acc: 0.9429\n",
            "Epoch 207/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1296 - acc: 0.9629 - val_loss: 0.2321 - val_acc: 0.9424\n",
            "Epoch 208/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1294 - acc: 0.9631 - val_loss: 0.2320 - val_acc: 0.9441\n",
            "Epoch 209/500\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 0.1298 - acc: 0.9623 - val_loss: 0.2303 - val_acc: 0.9440\n",
            "Epoch 210/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1293 - acc: 0.9632 - val_loss: 0.2318 - val_acc: 0.9432\n",
            "Epoch 211/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1293 - acc: 0.9633 - val_loss: 0.2314 - val_acc: 0.9446\n",
            "Epoch 212/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1291 - acc: 0.9634 - val_loss: 0.2317 - val_acc: 0.9426\n",
            "Epoch 213/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1288 - acc: 0.9630 - val_loss: 0.2298 - val_acc: 0.9435\n",
            "Epoch 214/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1289 - acc: 0.9633 - val_loss: 0.2322 - val_acc: 0.9431\n",
            "Epoch 215/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1287 - acc: 0.9630 - val_loss: 0.2335 - val_acc: 0.9433\n",
            "Epoch 216/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1287 - acc: 0.9630 - val_loss: 0.2331 - val_acc: 0.9433\n",
            "Epoch 217/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1286 - acc: 0.9636 - val_loss: 0.2322 - val_acc: 0.9435\n",
            "Epoch 218/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1285 - acc: 0.9629 - val_loss: 0.2327 - val_acc: 0.9427\n",
            "Epoch 219/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1284 - acc: 0.9639 - val_loss: 0.2370 - val_acc: 0.9430\n",
            "Epoch 220/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1282 - acc: 0.9630 - val_loss: 0.2348 - val_acc: 0.9443\n",
            "Epoch 221/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1281 - acc: 0.9634 - val_loss: 0.2324 - val_acc: 0.9441\n",
            "Epoch 222/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1283 - acc: 0.9632 - val_loss: 0.2336 - val_acc: 0.9448\n",
            "Epoch 223/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1281 - acc: 0.9639 - val_loss: 0.2355 - val_acc: 0.9431\n",
            "Epoch 224/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1279 - acc: 0.9633 - val_loss: 0.2334 - val_acc: 0.9441\n",
            "Epoch 225/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1278 - acc: 0.9632 - val_loss: 0.2342 - val_acc: 0.9440\n",
            "Epoch 226/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1276 - acc: 0.9632 - val_loss: 0.2352 - val_acc: 0.9428\n",
            "Epoch 227/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1275 - acc: 0.9632 - val_loss: 0.2369 - val_acc: 0.9432\n",
            "Epoch 228/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1275 - acc: 0.9633 - val_loss: 0.2379 - val_acc: 0.9431\n",
            "Epoch 229/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.1273 - acc: 0.9634 - val_loss: 0.2353 - val_acc: 0.9422\n",
            "Epoch 230/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1274 - acc: 0.9634 - val_loss: 0.2336 - val_acc: 0.9440\n",
            "Epoch 231/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1272 - acc: 0.9635 - val_loss: 0.2343 - val_acc: 0.9430\n",
            "Epoch 232/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1272 - acc: 0.9632 - val_loss: 0.2344 - val_acc: 0.9433\n",
            "Epoch 233/500\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.1265 - acc: 0.9637 - val_loss: 0.2371 - val_acc: 0.9431\n",
            "Epoch 234/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1268 - acc: 0.9639 - val_loss: 0.2374 - val_acc: 0.9422\n",
            "Epoch 235/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1265 - acc: 0.9633 - val_loss: 0.2356 - val_acc: 0.9417\n",
            "Epoch 236/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1266 - acc: 0.9635 - val_loss: 0.2355 - val_acc: 0.9442\n",
            "Epoch 237/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1263 - acc: 0.9638 - val_loss: 0.2370 - val_acc: 0.9425\n",
            "Epoch 238/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1268 - acc: 0.9638 - val_loss: 0.2366 - val_acc: 0.9427\n",
            "Epoch 239/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1267 - acc: 0.9636 - val_loss: 0.2354 - val_acc: 0.9428\n",
            "Epoch 240/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1263 - acc: 0.9634 - val_loss: 0.2379 - val_acc: 0.9432\n",
            "Epoch 241/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.1261 - acc: 0.9641 - val_loss: 0.2374 - val_acc: 0.9445\n",
            "Epoch 242/500\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 0.1259 - acc: 0.9642 - val_loss: 0.2366 - val_acc: 0.9431\n",
            "Epoch 243/500\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 0.1260 - acc: 0.9638 - val_loss: 0.2360 - val_acc: 0.9431\n",
            "Epoch 244/500\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 0.1262 - acc: 0.9638 - val_loss: 0.2371 - val_acc: 0.9431\n",
            "Epoch 245/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1256 - acc: 0.9637 - val_loss: 0.2385 - val_acc: 0.9432\n",
            "Epoch 246/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1259 - acc: 0.9640 - val_loss: 0.2361 - val_acc: 0.9437\n",
            "Epoch 247/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1255 - acc: 0.9640 - val_loss: 0.2372 - val_acc: 0.9425\n",
            "Epoch 248/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1253 - acc: 0.9638 - val_loss: 0.2372 - val_acc: 0.9435\n",
            "Epoch 249/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1255 - acc: 0.9639 - val_loss: 0.2386 - val_acc: 0.9436\n",
            "Epoch 250/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1252 - acc: 0.9642 - val_loss: 0.2393 - val_acc: 0.9427\n",
            "Epoch 251/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1254 - acc: 0.9637 - val_loss: 0.2404 - val_acc: 0.9426\n",
            "Epoch 252/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1250 - acc: 0.9647 - val_loss: 0.2394 - val_acc: 0.9440\n",
            "Epoch 253/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1251 - acc: 0.9638 - val_loss: 0.2399 - val_acc: 0.9410\n",
            "Epoch 254/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1253 - acc: 0.9642 - val_loss: 0.2385 - val_acc: 0.9437\n",
            "Epoch 255/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1247 - acc: 0.9647 - val_loss: 0.2396 - val_acc: 0.9419\n",
            "Epoch 256/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1243 - acc: 0.9647 - val_loss: 0.2431 - val_acc: 0.9421\n",
            "Epoch 257/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1250 - acc: 0.9640 - val_loss: 0.2385 - val_acc: 0.9420\n",
            "Epoch 258/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1244 - acc: 0.9643 - val_loss: 0.2401 - val_acc: 0.9419\n",
            "Epoch 259/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1247 - acc: 0.9643 - val_loss: 0.2404 - val_acc: 0.9433\n",
            "Epoch 260/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1244 - acc: 0.9646 - val_loss: 0.2408 - val_acc: 0.9423\n",
            "Epoch 261/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1240 - acc: 0.9648 - val_loss: 0.2402 - val_acc: 0.9426\n",
            "Epoch 262/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1240 - acc: 0.9645 - val_loss: 0.2404 - val_acc: 0.9436\n",
            "Epoch 263/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1245 - acc: 0.9639 - val_loss: 0.2407 - val_acc: 0.9429\n",
            "Epoch 264/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1244 - acc: 0.9639 - val_loss: 0.2398 - val_acc: 0.9425\n",
            "Epoch 265/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1240 - acc: 0.9648 - val_loss: 0.2417 - val_acc: 0.9431\n",
            "Epoch 266/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1241 - acc: 0.9645 - val_loss: 0.2433 - val_acc: 0.9419\n",
            "Epoch 267/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.1238 - acc: 0.9646 - val_loss: 0.2395 - val_acc: 0.9429\n",
            "Epoch 268/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1238 - acc: 0.9649 - val_loss: 0.2417 - val_acc: 0.9425\n",
            "Epoch 269/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1236 - acc: 0.9646 - val_loss: 0.2417 - val_acc: 0.9428\n",
            "Epoch 270/500\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.1236 - acc: 0.9646 - val_loss: 0.2411 - val_acc: 0.9439\n",
            "Epoch 271/500\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.1236 - acc: 0.9645 - val_loss: 0.2424 - val_acc: 0.9420\n",
            "Epoch 272/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1240 - acc: 0.9646 - val_loss: 0.2415 - val_acc: 0.9434\n",
            "Epoch 273/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1232 - acc: 0.9647 - val_loss: 0.2428 - val_acc: 0.9430\n",
            "Epoch 274/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1236 - acc: 0.9642 - val_loss: 0.2418 - val_acc: 0.9435\n",
            "Epoch 275/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1231 - acc: 0.9644 - val_loss: 0.2420 - val_acc: 0.9417\n",
            "Epoch 276/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1232 - acc: 0.9650 - val_loss: 0.2424 - val_acc: 0.9421\n",
            "Epoch 277/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1230 - acc: 0.9648 - val_loss: 0.2448 - val_acc: 0.9422\n",
            "Epoch 278/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1231 - acc: 0.9645 - val_loss: 0.2421 - val_acc: 0.9423\n",
            "Epoch 279/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1228 - acc: 0.9644 - val_loss: 0.2436 - val_acc: 0.9423\n",
            "Epoch 280/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1226 - acc: 0.9643 - val_loss: 0.2420 - val_acc: 0.9440\n",
            "Epoch 281/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1231 - acc: 0.9647 - val_loss: 0.2432 - val_acc: 0.9421\n",
            "Epoch 282/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1227 - acc: 0.9653 - val_loss: 0.2438 - val_acc: 0.9416\n",
            "Epoch 283/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1229 - acc: 0.9649 - val_loss: 0.2427 - val_acc: 0.9434\n",
            "Epoch 284/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1225 - acc: 0.9649 - val_loss: 0.2437 - val_acc: 0.9424\n",
            "Epoch 285/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1227 - acc: 0.9646 - val_loss: 0.2449 - val_acc: 0.9425\n",
            "Epoch 286/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1229 - acc: 0.9646 - val_loss: 0.2431 - val_acc: 0.9430\n",
            "Epoch 287/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1226 - acc: 0.9647 - val_loss: 0.2426 - val_acc: 0.9425\n",
            "Epoch 288/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1225 - acc: 0.9652 - val_loss: 0.2426 - val_acc: 0.9436\n",
            "Epoch 289/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1225 - acc: 0.9645 - val_loss: 0.2480 - val_acc: 0.9418\n",
            "Epoch 290/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1224 - acc: 0.9649 - val_loss: 0.2451 - val_acc: 0.9413\n",
            "Epoch 291/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1223 - acc: 0.9647 - val_loss: 0.2425 - val_acc: 0.9423\n",
            "Epoch 292/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1221 - acc: 0.9649 - val_loss: 0.2452 - val_acc: 0.9408\n",
            "Epoch 293/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1221 - acc: 0.9651 - val_loss: 0.2479 - val_acc: 0.9405\n",
            "Epoch 294/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1221 - acc: 0.9645 - val_loss: 0.2454 - val_acc: 0.9420\n",
            "Epoch 295/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1219 - acc: 0.9646 - val_loss: 0.2446 - val_acc: 0.9430\n",
            "Epoch 296/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1218 - acc: 0.9649 - val_loss: 0.2446 - val_acc: 0.9413\n",
            "Epoch 297/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1218 - acc: 0.9650 - val_loss: 0.2478 - val_acc: 0.9405\n",
            "Epoch 298/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1217 - acc: 0.9647 - val_loss: 0.2457 - val_acc: 0.9429\n",
            "Epoch 299/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1216 - acc: 0.9652 - val_loss: 0.2474 - val_acc: 0.9421\n",
            "Epoch 300/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1216 - acc: 0.9653 - val_loss: 0.2486 - val_acc: 0.9408\n",
            "Epoch 301/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1219 - acc: 0.9652 - val_loss: 0.2472 - val_acc: 0.9419\n",
            "Epoch 302/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1216 - acc: 0.9652 - val_loss: 0.2484 - val_acc: 0.9423\n",
            "Epoch 303/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1216 - acc: 0.9651 - val_loss: 0.2478 - val_acc: 0.9421\n",
            "Epoch 304/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1217 - acc: 0.9650 - val_loss: 0.2471 - val_acc: 0.9422\n",
            "Epoch 305/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1213 - acc: 0.9649 - val_loss: 0.2483 - val_acc: 0.9421\n",
            "Epoch 306/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.1213 - acc: 0.9651 - val_loss: 0.2472 - val_acc: 0.9414\n",
            "Epoch 307/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1210 - acc: 0.9655 - val_loss: 0.2460 - val_acc: 0.9403\n",
            "Epoch 308/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1212 - acc: 0.9651 - val_loss: 0.2480 - val_acc: 0.9406\n",
            "Epoch 309/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1212 - acc: 0.9656 - val_loss: 0.2478 - val_acc: 0.9409\n",
            "Epoch 310/500\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.1210 - acc: 0.9655 - val_loss: 0.2470 - val_acc: 0.9416\n",
            "Epoch 311/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1210 - acc: 0.9650 - val_loss: 0.2471 - val_acc: 0.9413\n",
            "Epoch 312/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1211 - acc: 0.9655 - val_loss: 0.2478 - val_acc: 0.9418\n",
            "Epoch 313/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1210 - acc: 0.9647 - val_loss: 0.2494 - val_acc: 0.9421\n",
            "Epoch 314/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1206 - acc: 0.9655 - val_loss: 0.2483 - val_acc: 0.9408\n",
            "Epoch 315/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1208 - acc: 0.9652 - val_loss: 0.2491 - val_acc: 0.9412\n",
            "Epoch 316/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1205 - acc: 0.9653 - val_loss: 0.2490 - val_acc: 0.9418\n",
            "Epoch 317/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1207 - acc: 0.9655 - val_loss: 0.2509 - val_acc: 0.9420\n",
            "Epoch 318/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1204 - acc: 0.9656 - val_loss: 0.2517 - val_acc: 0.9418\n",
            "Epoch 319/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1208 - acc: 0.9653 - val_loss: 0.2482 - val_acc: 0.9410\n",
            "Epoch 320/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1203 - acc: 0.9654 - val_loss: 0.2481 - val_acc: 0.9421\n",
            "Epoch 321/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1203 - acc: 0.9658 - val_loss: 0.2493 - val_acc: 0.9417\n",
            "Epoch 322/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1203 - acc: 0.9654 - val_loss: 0.2507 - val_acc: 0.9416\n",
            "Epoch 323/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1202 - acc: 0.9653 - val_loss: 0.2500 - val_acc: 0.9421\n",
            "Epoch 324/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1202 - acc: 0.9661 - val_loss: 0.2500 - val_acc: 0.9411\n",
            "Epoch 325/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1202 - acc: 0.9659 - val_loss: 0.2502 - val_acc: 0.9415\n",
            "Epoch 326/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1200 - acc: 0.9657 - val_loss: 0.2507 - val_acc: 0.9424\n",
            "Epoch 327/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1200 - acc: 0.9660 - val_loss: 0.2511 - val_acc: 0.9424\n",
            "Epoch 328/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1200 - acc: 0.9660 - val_loss: 0.2517 - val_acc: 0.9404\n",
            "Epoch 329/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1200 - acc: 0.9659 - val_loss: 0.2535 - val_acc: 0.9419\n",
            "Epoch 330/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1196 - acc: 0.9651 - val_loss: 0.2496 - val_acc: 0.9408\n",
            "Epoch 331/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1200 - acc: 0.9655 - val_loss: 0.2513 - val_acc: 0.9413\n",
            "Epoch 332/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1195 - acc: 0.9657 - val_loss: 0.2502 - val_acc: 0.9412\n",
            "Epoch 333/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1198 - acc: 0.9657 - val_loss: 0.2497 - val_acc: 0.9394\n",
            "Epoch 334/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.1199 - acc: 0.9654 - val_loss: 0.2520 - val_acc: 0.9408\n",
            "Epoch 335/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1198 - acc: 0.9655 - val_loss: 0.2514 - val_acc: 0.9402\n",
            "Epoch 336/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1196 - acc: 0.9654 - val_loss: 0.2540 - val_acc: 0.9419\n",
            "Epoch 337/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1197 - acc: 0.9658 - val_loss: 0.2563 - val_acc: 0.9409\n",
            "Epoch 338/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.1194 - acc: 0.9659 - val_loss: 0.2536 - val_acc: 0.9411\n",
            "Epoch 339/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1193 - acc: 0.9653 - val_loss: 0.2514 - val_acc: 0.9417\n",
            "Epoch 340/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1192 - acc: 0.9655 - val_loss: 0.2514 - val_acc: 0.9414\n",
            "Epoch 341/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1191 - acc: 0.9658 - val_loss: 0.2505 - val_acc: 0.9415\n",
            "Epoch 342/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1191 - acc: 0.9657 - val_loss: 0.2565 - val_acc: 0.9413\n",
            "Epoch 343/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1194 - acc: 0.9663 - val_loss: 0.2510 - val_acc: 0.9417\n",
            "Epoch 344/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1189 - acc: 0.9658 - val_loss: 0.2547 - val_acc: 0.9420\n",
            "Epoch 345/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.1190 - acc: 0.9655 - val_loss: 0.2515 - val_acc: 0.9410\n",
            "Epoch 346/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1190 - acc: 0.9660 - val_loss: 0.2542 - val_acc: 0.9406\n",
            "Epoch 347/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1190 - acc: 0.9661 - val_loss: 0.2581 - val_acc: 0.9399\n",
            "Epoch 348/500\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.1189 - acc: 0.9660 - val_loss: 0.2523 - val_acc: 0.9408\n",
            "Epoch 349/500\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.1189 - acc: 0.9659 - val_loss: 0.2546 - val_acc: 0.9401\n",
            "Epoch 350/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1191 - acc: 0.9660 - val_loss: 0.2543 - val_acc: 0.9404\n",
            "Epoch 351/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1186 - acc: 0.9658 - val_loss: 0.2546 - val_acc: 0.9409\n",
            "Epoch 352/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1189 - acc: 0.9661 - val_loss: 0.2524 - val_acc: 0.9407\n",
            "Epoch 353/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1184 - acc: 0.9661 - val_loss: 0.2534 - val_acc: 0.9408\n",
            "Epoch 354/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.1189 - acc: 0.9657 - val_loss: 0.2567 - val_acc: 0.9396\n",
            "Epoch 355/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1186 - acc: 0.9657 - val_loss: 0.2554 - val_acc: 0.9417\n",
            "Epoch 356/500\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 0.1186 - acc: 0.9658 - val_loss: 0.2575 - val_acc: 0.9409\n",
            "Epoch 357/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.1185 - acc: 0.9665 - val_loss: 0.2536 - val_acc: 0.9416\n",
            "Epoch 358/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1186 - acc: 0.9663 - val_loss: 0.2539 - val_acc: 0.9407\n",
            "Epoch 359/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1182 - acc: 0.9659 - val_loss: 0.2549 - val_acc: 0.9404\n",
            "Epoch 360/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1183 - acc: 0.9660 - val_loss: 0.2539 - val_acc: 0.9401\n",
            "Epoch 361/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1185 - acc: 0.9661 - val_loss: 0.2542 - val_acc: 0.9414\n",
            "Epoch 362/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1181 - acc: 0.9662 - val_loss: 0.2548 - val_acc: 0.9412\n",
            "Epoch 363/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1182 - acc: 0.9661 - val_loss: 0.2567 - val_acc: 0.9391\n",
            "Epoch 364/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.1183 - acc: 0.9664 - val_loss: 0.2541 - val_acc: 0.9422\n",
            "Epoch 365/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1180 - acc: 0.9663 - val_loss: 0.2556 - val_acc: 0.9399\n",
            "Epoch 366/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1180 - acc: 0.9659 - val_loss: 0.2572 - val_acc: 0.9409\n",
            "Epoch 367/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1178 - acc: 0.9661 - val_loss: 0.2560 - val_acc: 0.9403\n",
            "Epoch 368/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1182 - acc: 0.9661 - val_loss: 0.2560 - val_acc: 0.9392\n",
            "Epoch 369/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1180 - acc: 0.9662 - val_loss: 0.2573 - val_acc: 0.9409\n",
            "Epoch 370/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1185 - acc: 0.9661 - val_loss: 0.2554 - val_acc: 0.9406\n",
            "Epoch 371/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.1179 - acc: 0.9662 - val_loss: 0.2570 - val_acc: 0.9409\n",
            "Epoch 372/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1177 - acc: 0.9666 - val_loss: 0.2570 - val_acc: 0.9418\n",
            "Epoch 373/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1183 - acc: 0.9661 - val_loss: 0.2585 - val_acc: 0.9410\n",
            "Epoch 374/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1180 - acc: 0.9662 - val_loss: 0.2593 - val_acc: 0.9404\n",
            "Epoch 375/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1176 - acc: 0.9663 - val_loss: 0.2585 - val_acc: 0.9407\n",
            "Epoch 376/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1177 - acc: 0.9662 - val_loss: 0.2576 - val_acc: 0.9388\n",
            "Epoch 377/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1179 - acc: 0.9661 - val_loss: 0.2564 - val_acc: 0.9414\n",
            "Epoch 378/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1174 - acc: 0.9663 - val_loss: 0.2583 - val_acc: 0.9406\n",
            "Epoch 379/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1174 - acc: 0.9661 - val_loss: 0.2626 - val_acc: 0.9387\n",
            "Epoch 380/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1172 - acc: 0.9667 - val_loss: 0.2590 - val_acc: 0.9406\n",
            "Epoch 381/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1175 - acc: 0.9662 - val_loss: 0.2589 - val_acc: 0.9411\n",
            "Epoch 382/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1172 - acc: 0.9657 - val_loss: 0.2599 - val_acc: 0.9403\n",
            "Epoch 383/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1173 - acc: 0.9665 - val_loss: 0.2582 - val_acc: 0.9394\n",
            "Epoch 384/500\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.1172 - acc: 0.9663 - val_loss: 0.2605 - val_acc: 0.9396\n",
            "Epoch 385/500\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.1171 - acc: 0.9663 - val_loss: 0.2579 - val_acc: 0.9404\n",
            "Epoch 386/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1170 - acc: 0.9658 - val_loss: 0.2570 - val_acc: 0.9406\n",
            "Epoch 387/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1171 - acc: 0.9667 - val_loss: 0.2585 - val_acc: 0.9403\n",
            "Epoch 388/500\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.1169 - acc: 0.9669 - val_loss: 0.2589 - val_acc: 0.9411\n",
            "Epoch 389/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1172 - acc: 0.9658 - val_loss: 0.2574 - val_acc: 0.9400\n",
            "Epoch 390/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1171 - acc: 0.9664 - val_loss: 0.2618 - val_acc: 0.9394\n",
            "Epoch 391/500\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 0.1170 - acc: 0.9666 - val_loss: 0.2617 - val_acc: 0.9397\n",
            "Epoch 392/500\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 0.1175 - acc: 0.9662 - val_loss: 0.2592 - val_acc: 0.9405\n",
            "Epoch 393/500\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 0.1170 - acc: 0.9663 - val_loss: 0.2607 - val_acc: 0.9392\n",
            "Epoch 394/500\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 0.1168 - acc: 0.9666 - val_loss: 0.2600 - val_acc: 0.9398\n",
            "Epoch 395/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1169 - acc: 0.9664 - val_loss: 0.2580 - val_acc: 0.9396\n",
            "Epoch 396/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1169 - acc: 0.9665 - val_loss: 0.2628 - val_acc: 0.9399\n",
            "Epoch 397/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1166 - acc: 0.9663 - val_loss: 0.2589 - val_acc: 0.9414\n",
            "Epoch 398/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1162 - acc: 0.9665 - val_loss: 0.2613 - val_acc: 0.9391\n",
            "Epoch 399/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1168 - acc: 0.9668 - val_loss: 0.2619 - val_acc: 0.9384\n",
            "Epoch 400/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1166 - acc: 0.9663 - val_loss: 0.2598 - val_acc: 0.9400\n",
            "Epoch 401/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1165 - acc: 0.9666 - val_loss: 0.2608 - val_acc: 0.9399\n",
            "Epoch 402/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1165 - acc: 0.9663 - val_loss: 0.2605 - val_acc: 0.9412\n",
            "Epoch 403/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1167 - acc: 0.9665 - val_loss: 0.2606 - val_acc: 0.9401\n",
            "Epoch 404/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1164 - acc: 0.9667 - val_loss: 0.2611 - val_acc: 0.9402\n",
            "Epoch 405/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1162 - acc: 0.9663 - val_loss: 0.2603 - val_acc: 0.9402\n",
            "Epoch 406/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1161 - acc: 0.9665 - val_loss: 0.2607 - val_acc: 0.9400\n",
            "Epoch 407/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.1163 - acc: 0.9670 - val_loss: 0.2637 - val_acc: 0.9402\n",
            "Epoch 408/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1165 - acc: 0.9664 - val_loss: 0.2606 - val_acc: 0.9390\n",
            "Epoch 409/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1161 - acc: 0.9664 - val_loss: 0.2619 - val_acc: 0.9407\n",
            "Epoch 410/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1162 - acc: 0.9664 - val_loss: 0.2619 - val_acc: 0.9409\n",
            "Epoch 411/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1162 - acc: 0.9665 - val_loss: 0.2602 - val_acc: 0.9391\n",
            "Epoch 412/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.1159 - acc: 0.9666 - val_loss: 0.2640 - val_acc: 0.9403\n",
            "Epoch 413/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1158 - acc: 0.9666 - val_loss: 0.2624 - val_acc: 0.9399\n",
            "Epoch 414/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.1159 - acc: 0.9664 - val_loss: 0.2627 - val_acc: 0.9394\n",
            "Epoch 415/500\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.1158 - acc: 0.9671 - val_loss: 0.2616 - val_acc: 0.9393\n",
            "Epoch 416/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.1158 - acc: 0.9673 - val_loss: 0.2623 - val_acc: 0.9407\n",
            "Epoch 417/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1161 - acc: 0.9666 - val_loss: 0.2631 - val_acc: 0.9402\n",
            "Epoch 418/500\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.1157 - acc: 0.9665 - val_loss: 0.2631 - val_acc: 0.9382\n",
            "Epoch 419/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.1160 - acc: 0.9668 - val_loss: 0.2635 - val_acc: 0.9397\n",
            "Epoch 420/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.1159 - acc: 0.9665 - val_loss: 0.2649 - val_acc: 0.9405\n",
            "Epoch 421/500\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.1157 - acc: 0.9671 - val_loss: 0.2647 - val_acc: 0.9390\n",
            "Epoch 422/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1156 - acc: 0.9668 - val_loss: 0.2634 - val_acc: 0.9391\n",
            "Epoch 423/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1158 - acc: 0.9667 - val_loss: 0.2629 - val_acc: 0.9397\n",
            "Epoch 424/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1154 - acc: 0.9670 - val_loss: 0.2641 - val_acc: 0.9395\n",
            "Epoch 425/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1158 - acc: 0.9666 - val_loss: 0.2627 - val_acc: 0.9401\n",
            "Epoch 426/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1157 - acc: 0.9670 - val_loss: 0.2657 - val_acc: 0.9380\n",
            "Epoch 427/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1157 - acc: 0.9669 - val_loss: 0.2650 - val_acc: 0.9403\n",
            "Epoch 428/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1153 - acc: 0.9667 - val_loss: 0.2621 - val_acc: 0.9401\n",
            "Epoch 429/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1154 - acc: 0.9667 - val_loss: 0.2652 - val_acc: 0.9402\n",
            "Epoch 430/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1157 - acc: 0.9666 - val_loss: 0.2659 - val_acc: 0.9401\n",
            "Epoch 431/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1155 - acc: 0.9670 - val_loss: 0.2636 - val_acc: 0.9395\n",
            "Epoch 432/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1151 - acc: 0.9673 - val_loss: 0.2668 - val_acc: 0.9399\n",
            "Epoch 433/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1153 - acc: 0.9670 - val_loss: 0.2635 - val_acc: 0.9380\n",
            "Epoch 434/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1153 - acc: 0.9667 - val_loss: 0.2647 - val_acc: 0.9388\n",
            "Epoch 435/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1153 - acc: 0.9669 - val_loss: 0.2631 - val_acc: 0.9391\n",
            "Epoch 436/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1151 - acc: 0.9669 - val_loss: 0.2646 - val_acc: 0.9383\n",
            "Epoch 437/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1151 - acc: 0.9673 - val_loss: 0.2631 - val_acc: 0.9391\n",
            "Epoch 438/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1148 - acc: 0.9677 - val_loss: 0.2656 - val_acc: 0.9383\n",
            "Epoch 439/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1153 - acc: 0.9668 - val_loss: 0.2647 - val_acc: 0.9392\n",
            "Epoch 440/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1150 - acc: 0.9670 - val_loss: 0.2677 - val_acc: 0.9388\n",
            "Epoch 441/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1153 - acc: 0.9666 - val_loss: 0.2663 - val_acc: 0.9392\n",
            "Epoch 442/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1149 - acc: 0.9670 - val_loss: 0.2651 - val_acc: 0.9400\n",
            "Epoch 443/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1148 - acc: 0.9669 - val_loss: 0.2682 - val_acc: 0.9407\n",
            "Epoch 444/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1150 - acc: 0.9667 - val_loss: 0.2657 - val_acc: 0.9398\n",
            "Epoch 445/500\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.1151 - acc: 0.9669 - val_loss: 0.2661 - val_acc: 0.9397\n",
            "Epoch 446/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1147 - acc: 0.9672 - val_loss: 0.2670 - val_acc: 0.9394\n",
            "Epoch 447/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1150 - acc: 0.9668 - val_loss: 0.2655 - val_acc: 0.9391\n",
            "Epoch 448/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1152 - acc: 0.9666 - val_loss: 0.2678 - val_acc: 0.9407\n",
            "Epoch 449/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1148 - acc: 0.9673 - val_loss: 0.2688 - val_acc: 0.9389\n",
            "Epoch 450/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1148 - acc: 0.9671 - val_loss: 0.2652 - val_acc: 0.9396\n",
            "Epoch 451/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1146 - acc: 0.9666 - val_loss: 0.2661 - val_acc: 0.9397\n",
            "Epoch 452/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1144 - acc: 0.9670 - val_loss: 0.2676 - val_acc: 0.9387\n",
            "Epoch 453/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1143 - acc: 0.9672 - val_loss: 0.2664 - val_acc: 0.9387\n",
            "Epoch 454/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1147 - acc: 0.9671 - val_loss: 0.2685 - val_acc: 0.9391\n",
            "Epoch 455/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1145 - acc: 0.9669 - val_loss: 0.2698 - val_acc: 0.9385\n",
            "Epoch 456/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1146 - acc: 0.9668 - val_loss: 0.2673 - val_acc: 0.9385\n",
            "Epoch 457/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1142 - acc: 0.9672 - val_loss: 0.2678 - val_acc: 0.9409\n",
            "Epoch 458/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1147 - acc: 0.9670 - val_loss: 0.2672 - val_acc: 0.9394\n",
            "Epoch 459/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1145 - acc: 0.9671 - val_loss: 0.2697 - val_acc: 0.9399\n",
            "Epoch 460/500\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.1147 - acc: 0.9669 - val_loss: 0.2683 - val_acc: 0.9386\n",
            "Epoch 461/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1144 - acc: 0.9667 - val_loss: 0.2689 - val_acc: 0.9404\n",
            "Epoch 462/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1141 - acc: 0.9673 - val_loss: 0.2704 - val_acc: 0.9390\n",
            "Epoch 463/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1144 - acc: 0.9672 - val_loss: 0.2695 - val_acc: 0.9391\n",
            "Epoch 464/500\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.1143 - acc: 0.9673 - val_loss: 0.2696 - val_acc: 0.9382\n",
            "Epoch 465/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1140 - acc: 0.9670 - val_loss: 0.2693 - val_acc: 0.9387\n",
            "Epoch 466/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1140 - acc: 0.9670 - val_loss: 0.2692 - val_acc: 0.9383\n",
            "Epoch 467/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1141 - acc: 0.9669 - val_loss: 0.2677 - val_acc: 0.9397\n",
            "Epoch 468/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1145 - acc: 0.9665 - val_loss: 0.2727 - val_acc: 0.9390\n",
            "Epoch 469/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1140 - acc: 0.9670 - val_loss: 0.2692 - val_acc: 0.9394\n",
            "Epoch 470/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1137 - acc: 0.9676 - val_loss: 0.2737 - val_acc: 0.9376\n",
            "Epoch 471/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1139 - acc: 0.9672 - val_loss: 0.2690 - val_acc: 0.9384\n",
            "Epoch 472/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1141 - acc: 0.9669 - val_loss: 0.2705 - val_acc: 0.9378\n",
            "Epoch 473/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1137 - acc: 0.9672 - val_loss: 0.2711 - val_acc: 0.9377\n",
            "Epoch 474/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1138 - acc: 0.9673 - val_loss: 0.2697 - val_acc: 0.9390\n",
            "Epoch 475/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1140 - acc: 0.9667 - val_loss: 0.2708 - val_acc: 0.9378\n",
            "Epoch 476/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1139 - acc: 0.9675 - val_loss: 0.2702 - val_acc: 0.9395\n",
            "Epoch 477/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1133 - acc: 0.9675 - val_loss: 0.2728 - val_acc: 0.9395\n",
            "Epoch 478/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1139 - acc: 0.9672 - val_loss: 0.2703 - val_acc: 0.9381\n",
            "Epoch 479/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1139 - acc: 0.9673 - val_loss: 0.2702 - val_acc: 0.9367\n",
            "Epoch 480/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1137 - acc: 0.9671 - val_loss: 0.2686 - val_acc: 0.9392\n",
            "Epoch 481/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1137 - acc: 0.9672 - val_loss: 0.2709 - val_acc: 0.9379\n",
            "Epoch 482/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1138 - acc: 0.9671 - val_loss: 0.2717 - val_acc: 0.9397\n",
            "Epoch 483/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1136 - acc: 0.9671 - val_loss: 0.2707 - val_acc: 0.9395\n",
            "Epoch 484/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1136 - acc: 0.9675 - val_loss: 0.2732 - val_acc: 0.9377\n",
            "Epoch 485/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1135 - acc: 0.9678 - val_loss: 0.2725 - val_acc: 0.9370\n",
            "Epoch 486/500\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.1133 - acc: 0.9672 - val_loss: 0.2742 - val_acc: 0.9388\n",
            "Epoch 487/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1134 - acc: 0.9675 - val_loss: 0.2730 - val_acc: 0.9389\n",
            "Epoch 488/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1137 - acc: 0.9671 - val_loss: 0.2725 - val_acc: 0.9394\n",
            "Epoch 489/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1136 - acc: 0.9674 - val_loss: 0.2707 - val_acc: 0.9385\n",
            "Epoch 490/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1134 - acc: 0.9676 - val_loss: 0.2725 - val_acc: 0.9387\n",
            "Epoch 491/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1135 - acc: 0.9673 - val_loss: 0.2740 - val_acc: 0.9371\n",
            "Epoch 492/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1133 - acc: 0.9673 - val_loss: 0.2722 - val_acc: 0.9386\n",
            "Epoch 493/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1132 - acc: 0.9676 - val_loss: 0.2718 - val_acc: 0.9382\n",
            "Epoch 494/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1133 - acc: 0.9677 - val_loss: 0.2730 - val_acc: 0.9376\n",
            "Epoch 495/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1129 - acc: 0.9674 - val_loss: 0.2751 - val_acc: 0.9390\n",
            "Epoch 496/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1134 - acc: 0.9674 - val_loss: 0.2744 - val_acc: 0.9386\n",
            "Epoch 497/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1133 - acc: 0.9672 - val_loss: 0.2727 - val_acc: 0.9382\n",
            "Epoch 498/500\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.1130 - acc: 0.9680 - val_loss: 0.2731 - val_acc: 0.9390\n",
            "Epoch 499/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1129 - acc: 0.9679 - val_loss: 0.2760 - val_acc: 0.9379\n",
            "Epoch 500/500\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.1131 - acc: 0.9674 - val_loss: 0.2754 - val_acc: 0.9389\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_177 (Dense)            (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "activation_177 (Activation)  (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_178 (Dense)            (None, 10)                3010      \n",
            "_________________________________________________________________\n",
            "activation_178 (Activation)  (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 238,510\n",
            "Trainable params: 238,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.3857 - acc: 0.8979 - val_loss: 0.2056 - val_acc: 0.9421\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1740 - acc: 0.9515 - val_loss: 0.1419 - val_acc: 0.9592\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.1257 - acc: 0.9643 - val_loss: 0.1184 - val_acc: 0.9650\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0974 - acc: 0.9723 - val_loss: 0.0985 - val_acc: 0.9718\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0789 - acc: 0.9778 - val_loss: 0.0871 - val_acc: 0.9748\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0643 - acc: 0.9821 - val_loss: 0.0815 - val_acc: 0.9737\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0539 - acc: 0.9854 - val_loss: 0.0790 - val_acc: 0.9746\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0453 - acc: 0.9876 - val_loss: 0.0718 - val_acc: 0.9771\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0378 - acc: 0.9900 - val_loss: 0.0689 - val_acc: 0.9789\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0323 - acc: 0.9916 - val_loss: 0.0687 - val_acc: 0.9790\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0274 - acc: 0.9936 - val_loss: 0.0672 - val_acc: 0.9794\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0231 - acc: 0.9948 - val_loss: 0.0712 - val_acc: 0.9782\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0195 - acc: 0.9954 - val_loss: 0.0672 - val_acc: 0.9792\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0164 - acc: 0.9965 - val_loss: 0.0697 - val_acc: 0.9790\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0143 - acc: 0.9971 - val_loss: 0.0662 - val_acc: 0.9795\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0115 - acc: 0.9982 - val_loss: 0.0642 - val_acc: 0.9801\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0099 - acc: 0.9984 - val_loss: 0.0671 - val_acc: 0.9803\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0089 - acc: 0.9986 - val_loss: 0.0690 - val_acc: 0.9794\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0072 - acc: 0.9990 - val_loss: 0.0677 - val_acc: 0.9800\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0060 - acc: 0.9995 - val_loss: 0.0674 - val_acc: 0.9804\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_179 (Dense)            (None, 600)               471000    \n",
            "_________________________________________________________________\n",
            "activation_179 (Activation)  (None, 600)               0         \n",
            "_________________________________________________________________\n",
            "dense_180 (Dense)            (None, 10)                6010      \n",
            "_________________________________________________________________\n",
            "activation_180 (Activation)  (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 477,010\n",
            "Trainable params: 477,010\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.3334 - acc: 0.9098 - val_loss: 0.1864 - val_acc: 0.9482\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.1453 - acc: 0.9591 - val_loss: 0.1251 - val_acc: 0.9626\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.1002 - acc: 0.9715 - val_loss: 0.0949 - val_acc: 0.9722\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0738 - acc: 0.9790 - val_loss: 0.0786 - val_acc: 0.9772\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0574 - acc: 0.9838 - val_loss: 0.0776 - val_acc: 0.9758\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0453 - acc: 0.9875 - val_loss: 0.0756 - val_acc: 0.9780\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0361 - acc: 0.9905 - val_loss: 0.0665 - val_acc: 0.9792\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0285 - acc: 0.9930 - val_loss: 0.0705 - val_acc: 0.9781\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0231 - acc: 0.9946 - val_loss: 0.0658 - val_acc: 0.9804\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0183 - acc: 0.9960 - val_loss: 0.0627 - val_acc: 0.9808\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0153 - acc: 0.9968 - val_loss: 0.0657 - val_acc: 0.9806\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 0.0118 - acc: 0.9981 - val_loss: 0.0633 - val_acc: 0.9809\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.0104 - acc: 0.9981 - val_loss: 0.0623 - val_acc: 0.9815\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.0073 - acc: 0.9990 - val_loss: 0.0661 - val_acc: 0.9819\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.0062 - acc: 0.9992 - val_loss: 0.0629 - val_acc: 0.9810\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.0050 - acc: 0.9995 - val_loss: 0.0681 - val_acc: 0.9804\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0055 - acc: 0.9992 - val_loss: 0.0594 - val_acc: 0.9833\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0029 - acc: 0.9999 - val_loss: 0.0649 - val_acc: 0.9824\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0060 - acc: 0.9986 - val_loss: 0.0725 - val_acc: 0.9805\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0026 - acc: 0.9998 - val_loss: 0.0647 - val_acc: 0.9834\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_181 (Dense)            (None, 800)               628000    \n",
            "_________________________________________________________________\n",
            "activation_181 (Activation)  (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_182 (Dense)            (None, 10)                8010      \n",
            "_________________________________________________________________\n",
            "activation_182 (Activation)  (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 636,010\n",
            "Trainable params: 636,010\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.3122 - acc: 0.9151 - val_loss: 0.1637 - val_acc: 0.9526\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.1320 - acc: 0.9624 - val_loss: 0.1098 - val_acc: 0.9692\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0887 - acc: 0.9745 - val_loss: 0.0860 - val_acc: 0.9741\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0649 - acc: 0.9811 - val_loss: 0.0780 - val_acc: 0.9747\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0493 - acc: 0.9864 - val_loss: 0.0726 - val_acc: 0.9773\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0382 - acc: 0.9895 - val_loss: 0.0621 - val_acc: 0.9799\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0304 - acc: 0.9917 - val_loss: 0.0577 - val_acc: 0.9818\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0236 - acc: 0.9946 - val_loss: 0.0599 - val_acc: 0.9813\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0183 - acc: 0.9960 - val_loss: 0.0685 - val_acc: 0.9783\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0144 - acc: 0.9972 - val_loss: 0.0608 - val_acc: 0.9821\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0113 - acc: 0.9978 - val_loss: 0.0569 - val_acc: 0.9827\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0094 - acc: 0.9983 - val_loss: 0.0576 - val_acc: 0.9823\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0072 - acc: 0.9989 - val_loss: 0.0578 - val_acc: 0.9821\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0054 - acc: 0.9993 - val_loss: 0.0618 - val_acc: 0.9811\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0046 - acc: 0.9995 - val_loss: 0.0603 - val_acc: 0.9822\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0031 - acc: 0.9998 - val_loss: 0.0584 - val_acc: 0.9815\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0021 - acc: 0.9999 - val_loss: 0.0615 - val_acc: 0.9823\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0053 - acc: 0.9988 - val_loss: 0.0760 - val_acc: 0.9791\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0058 - acc: 0.9986 - val_loss: 0.0747 - val_acc: 0.9822\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0018 - acc: 0.9999 - val_loss: 0.0633 - val_acc: 0.9835\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_183 (Dense)            (None, 1000)              785000    \n",
            "_________________________________________________________________\n",
            "activation_183 (Activation)  (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_184 (Dense)            (None, 10)                10010     \n",
            "_________________________________________________________________\n",
            "activation_184 (Activation)  (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 795,010\n",
            "Trainable params: 795,010\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "57088/60000 [===========================>..] - ETA: 0s - loss: 0.3042 - acc: 0.9162"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aGAXb9-TwwCA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#To find the appropriate number of epochs for each model we plot the\n",
        "# history of the models.\n",
        "for dataset in ['numbers', 'fashion']:\n",
        "  data=DATA[dataset]\n",
        "  for i in range(len(data['hidden neurons'])):\n",
        "    plot_history(data['history'][i],'output of the fitting procedure for the '\n",
        "                 +dataset+' dataset with '+str(data['hidden neurons'][i])+\n",
        "                 ' hidden neurons.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hkhQkAPotsuc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#################### 4 ##################\n",
        "#plot the data\n",
        "for dataset in ['numbers','fashion']:\n",
        "\n",
        "    data=DATA[dataset]\n",
        "  \n",
        "    fig, ax1, ax2 = prepare_standardplot('Performance (loss and accuracy) by number of hidden neurons '\n",
        "                                         +'(dataset: '+dataset+')' ,\n",
        "                                         'hidden neurons')\n",
        "    ax1.plot(data['hidden neurons'],data[\"best validation loss\"], label =\"best loss\" )\n",
        "    ax2.plot(data['hidden neurons'],data[\"best validation accuracy\"], label = \"best accuracy\")\n",
        "    finalize_standardplot(fig, ax1, ax2)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fc_WrqMztsue",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Answer to question 2:"
      ]
    },
    {
      "metadata": {
        "id": "uKkmVbTk0oJ0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "RQrbMs8ntsug",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 5: Going deeper: tricks and regularization (8 points)\n",
        "\n",
        "### Description\n",
        "\n",
        "Adding hidden layers to a deep network does not necessarily lead to a straight-forward improvement of performance. Overfitting can be counteracted with regularization and dropout. Batch normalization is supposed to mainly speed up convergence. Since the MNIST dataset is almost perfectly solved already by a one-hidden-layer network we use the Fashion-MNIST dataset in this exercise.\n",
        "\n",
        "1. Add one or two hidden layers with 50 hidden neurons (each) and train the network for a sufficiently long time (at least 100 epochs). Since deep models are very expressive you will most probably encounter overfitting. Try to improve the best validation scores of the model (even if it is only a minor improvement) by experimenting with batch_normalization layers, dropout layers and l1- and l2-regularization on weights (kernels) and biases. (4 pts)\n",
        "2. After you have found good settings, plot the learning curves for both models, naive (=no tricks/regularization) and tuned (=tricks + regularized), preferably together in a comparison plot. Discuss your results; refer to the model performance with only 1 hidden layer. (2 sentences max.) (2pts)\n",
        "3. Fit your best performing (probably regularized deep) model also to MNIST for having a reference for the next exercise. Plot the resulting learning curves. (2 pts)\n",
        "\n",
        "### Solution"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-23T16:26:50.480763Z",
          "start_time": "2018-02-23T16:06:32.938435Z"
        },
        "scrolled": false,
        "id": "oS-7Yowmtsuh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# compute the data\n",
        "\n",
        "epochs= 2    #  <-------------------- set epochs here ! (at least 100)\n",
        "\n",
        "dataset='fashion'\n",
        "\n",
        "model_names=['normal','batch normalization', 'dropout 0.5', 'dropout 0.25', 'kernel regularizer l1',\n",
        "             'kernel regularizer l2',  'bias regularizer l1', 'bias regularizer l2', ]\n",
        "\n",
        "\n",
        "\n",
        "models=[\n",
        "    # normal\n",
        "    Sequential([\n",
        "        Dense(50, input_shape=(784,)),\n",
        "        Activation('relu'),\n",
        "        Dense(50, input_shape=(784,)),\n",
        "        Activation('relu'),\n",
        "        Dense(10),\n",
        "        Activation('softmax'),\n",
        "    ]),\n",
        "    # batch normalization\n",
        "    Sequential([\n",
        "        Dense(50, input_shape=(784,)),\n",
        "        BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \n",
        "                           center=True, scale=True, beta_initializer='zeros',\n",
        "                           gamma_initializer='ones', moving_mean_initializer='zeros',\n",
        "                           moving_variance_initializer='ones', beta_regularizer=None,\n",
        "                           gamma_regularizer=None, beta_constraint=None, gamma_constraint=None),\n",
        "        Activation('relu'),\n",
        "        Dense(50, input_shape=(784,)),\n",
        "        BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \n",
        "                           center=True, scale=True, beta_initializer='zeros',\n",
        "                           gamma_initializer='ones', moving_mean_initializer='zeros',\n",
        "                           moving_variance_initializer='ones', beta_regularizer=None,\n",
        "                           gamma_regularizer=None, beta_constraint=None, gamma_constraint=None),\n",
        "        Activation('relu'),\n",
        "        Dense(10),\n",
        "        Activation('softmax'),\n",
        "    ]),\n",
        "    # dropout 0.5\n",
        "    Sequential([\n",
        "        Dense(50, input_shape=(784,)),\n",
        "        Dropout(0.5),\n",
        "        Activation('relu'),\n",
        "        Dense(50, input_shape=(784,)),\n",
        "        Activation('relu'),\n",
        "        Dense(10),\n",
        "        Activation('softmax')\n",
        "    ]),\n",
        "    # dropout 0.25\n",
        "    Sequential([\n",
        "        Dense(50, input_shape=(784,)),\n",
        "        Dropout(0.25),\n",
        "        Activation('relu'),\n",
        "        Dense(50, input_shape=(784,)),\n",
        "        Activation('relu'),\n",
        "        Dense(10),\n",
        "        Activation('softmax')\n",
        "    ]),\n",
        "    # kernel reguralizer l1\n",
        "     Sequential([\n",
        "        Dense(50, input_shape=(784,),kernel_regularizer=regularizers.l1(0.01)),\n",
        "        Activation('relu'),\n",
        "        Dense(50, input_shape=(784,)),\n",
        "        Activation('relu'),\n",
        "        Dense(10),\n",
        "        Activation('softmax'),\n",
        "    ]),\n",
        "    # kernel reguralizer l2\n",
        "     Sequential([\n",
        "        Dense(50, input_shape=(784,),kernel_regularizer=regularizers.l2(0.01)),\n",
        "        Activation('relu'),\n",
        "        Dense(50, input_shape=(784,)),\n",
        "        Activation('relu'),\n",
        "        Dense(10),\n",
        "        Activation('softmax'),\n",
        "    ]),\n",
        "    # bias reguralizer l1\n",
        "     Sequential([\n",
        "        Dense(50, input_shape=(784,), bias_regularizer=regularizers.l2(0.01)),\n",
        "        Activation('relu'),\n",
        "        Dense(50, input_shape=(784,)),\n",
        "        Activation('relu'),\n",
        "        Dense(10),\n",
        "        Activation('softmax'),\n",
        "    ]),\n",
        "    # bias reguralizer l2\n",
        "     Sequential([\n",
        "        Dense(50, input_shape=(784,), bias_regularizer=regularizers.l2(0.01)),\n",
        "        Activation('relu'),\n",
        "        Dense(50, input_shape=(784,)),\n",
        "        Activation('relu'),\n",
        "        Dense(10),\n",
        "        Activation('softmax'),\n",
        "    ])\n",
        "    \n",
        "]\n",
        "\n",
        "historys=[]\n",
        "for model_key in range(len(models)):\n",
        "    adam5 = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999)\n",
        "    models[model_key].compile(optimizer=adam5,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['acc'])\n",
        "\n",
        "    h=models[model_key].fit(x_tr[dataset], y_tr[dataset], epochs=epochs,\n",
        "                                 batch_size=128, validation_data=(x_te[dataset], y_te[dataset]))\n",
        "\n",
        "    \n",
        "    historys+=[h.history]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "evSzlE24tsum",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Answer to question 2 (comments):"
      ]
    },
    {
      "metadata": {
        "id": "_pGxRjYktsun",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "DwH6-G1atsuo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "############ 5 ##################\n",
        "#plot the data\n",
        "  \n",
        "\n",
        "def multiple_comparison_plot(historys, model_names, title, training_or_validation='validation'):\n",
        "    fig, ax1, ax2 = prepare_standardplot(title, \"epochs\")\n",
        "    loss_key= 'val_loss' if training_or_validation=='validation' else 'loss'\n",
        "    acc_key= 'val_acc' if training_or_validation=='validation' else 'acc'\n",
        "\n",
        "    for model_key in range(len(historys)):\n",
        "        ax1.plot(historys[model_key][loss_key], label=model_names[model_key])\n",
        "        ax2.plot(historys[model_key][acc_key], label=model_names[model_key])\n",
        "    fig.set_size_inches(10,10)\n",
        "    finalize_standardplot(fig, ax1, ax2)\n",
        "    \n",
        "    \n",
        "multiple_comparison_plot(historys, model_names,'title', training_or_validation='validation' )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b_AFfTi7tsus",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 6: Convolutional neural networks (CNNs) (10 points)\n",
        "\n",
        "### Description\n",
        "\n",
        "Convolutional neural networks have an inductive bias that is well adapted to image classification.\n",
        "\n",
        "1. Design a convolutional neural network, play with different architectures and parameters. Hint: You may get valuable inspiration from the keras [examples](https://github.com/keras-team/keras/tree/master/examples). (4 pts)\n",
        "2. Plot the learning curves of the convolutional neural network for MNIST and Fashion-MNIST. (4 pts)\n",
        "3. How does the CNN performance compare to the so far best performing (deep) neural network model for the two data sets? (2 sentences max.) (2 pts)\n",
        "\n",
        "### Solution"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-23T16:05:21.840299Z",
          "start_time": "2018-02-23T15:51:11.993053Z"
        },
        "id": "ja4YIlRftsut",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################## 6.1 ####################\n",
        "\n",
        "# inspiration from https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
        "epochs= 300\n",
        "batch_size=128\n",
        "\n",
        "history={}\n",
        "for dataset in ['numbers', 'fashion']:\n",
        "\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                     activation='relu',\n",
        "                     input_shape=input_shape))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01) ))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                  optimizer= keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    history[dataset]=model.fit(x_tr[dataset], y_tr[dataset],\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              verbose=1,\n",
        "              validation_data=(x_te[dataset], y_te[dataset]))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0qCuoQYCtsuv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########### 6.2 ###########\n",
        "\n",
        "# plot the data\n",
        "\n",
        "\n",
        "comparison_plot(history['numbers'].history, history['fashion'].history, 'numbers', 'fashion', 'comparison of performance CNN numbers vs. fashion datasets')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TS8GMiedtsuy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Answer to question 3:"
      ]
    },
    {
      "metadata": {
        "id": "5UjF2E1gtsuz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "yAsCFeOItsuz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 7: Sigmoidal activation function and batch-normalization (6 points)\n",
        "\n",
        "### Description:\n",
        "\n",
        "In the original publication of batch normalization [Ioffe and Szegedy, 2014](https://arxiv.org/pdf/1502.03167.pdf), the authors mention a particularly beneficial effect of their method on networks with sigmoidal activation functions. This is because such networks usually suffer from saturating activations/vanishing gradients. Here we want to reproduce this behaviour (Chose either MNIST or Fashion-MNIST for this exercise).\n",
        "\n",
        "1. Implement the same convolutional network as in the previous exercise, but using the sigmoid activation function instead of the standard choice ReLU. Train the network for a reasonable amount of time. What do you observe? (1 sentence max.) (3 pts)\n",
        "2. Add batch-normalization layers to all convolutional and fully-connected layers (i.e. before each layer with learnable parameters). How does the performance change? Can the network reach the ReLU-CNN performance of the previous exercise? (1 sentence max.) (3 pts)\n",
        "3. **BONUS (optional, not graded**): Investigate our initial guess that saturating activity/vanishing gradients might be the cause of this behaviour. For that, create histograms of the hidden activitions for different hidden layers for the sigmoid-CNN and the sigmoid-CNN with batch-normalization (counting over both, samples and neurons per layer). You may only chose layers with learnable parameters. What do you observe?\n",
        "Hint: You can use the [keract](https://github.com/philipperemy/keract) package to access neural activation values for all layers of your network model.\n",
        "\n",
        "\n",
        "\n",
        "### Solution:"
      ]
    },
    {
      "metadata": {
        "id": "SMj5h6rNtsu0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "######################## 7.1 ######################\n",
        "epochs= 300\n",
        "batch_size=128\n",
        "\n",
        "dataset='fashion'\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='sigmoit',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='sigmoid'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01) ))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer= keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history=model.fit(x_tr[dataset], y_tr[dataset],\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_te[dataset], y_te[dataset]))\n",
        "\n",
        "print(dataset)\n",
        "print(history.history)\n",
        "with open('./7/history7.1.json', 'w') as outfile:\n",
        "    json.dump(history, outfile)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h1NV3vFDtsu9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "historys=[]\n",
        "model_types=['relu', 'sigmoid']\n",
        "\n",
        "    \n",
        "with open('./6/history_fashion.json', 'r') as infile:\n",
        "    historys+=[json.load(infile)]\n",
        "    \n",
        "with open('./7/history7.1.json', 'r') as infile:\n",
        "    historys+=[json.load(infile)]\n",
        "\n",
        "        \n",
        "\n",
        "comparison_plot(history7, history6, 'numbers', 'fashion', 'comparison of performance CNN numbers vs. fashion datasets')\n",
        "\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E0C76OrvtsvB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################# 7.2 #####################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P-rrlXXCtsvG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Answer to question 1:"
      ]
    },
    {
      "metadata": {
        "id": "kTtvgg9TtsvH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "DXFKr1uTtsvH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Answer to question 2:"
      ]
    },
    {
      "metadata": {
        "id": "dglRhAggtsvI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}